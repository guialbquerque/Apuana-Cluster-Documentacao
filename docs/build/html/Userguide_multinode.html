
<!DOCTYPE html>

<html lang="pt-BR">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Multiple Nodes &#8212; documentação Cluster Cin latest</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/groundwork.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/translations.js"></script>
    <link rel="index" title="Índice" href="genindex.html" />
    <link rel="search" title="Buscar" href="search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navegação</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="Índice Geral"
             accesskey="I">índice</a></li>
        <li class="nav-item nav-item-0"><a href="index.html">documentação Cluster Cin latest</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Multiple Nodes</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="multiple-nodes">
<h1>Multiple Nodes<a class="headerlink" href="#multiple-nodes" title="Link permanente para este cabeçalho">¶</a></h1>
<section id="data-parallel">
<h2>Data Parallel<a class="headerlink" href="#data-parallel" title="Link permanente para este cabeçalho">¶</a></h2>
<img alt="_images/dataparallel.png" src="_images/dataparallel.png" />
<p>Request 3 nodes with at least 4 GPUs each.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>!/bin/bash

<span class="gp"># </span>Number of Nodes
<span class="gp">#</span>SBATCH --nodes<span class="o">=</span><span class="m">3</span>

<span class="gp"># </span>Number of tasks. <span class="m">3</span> <span class="o">(</span><span class="m">1</span> per node<span class="o">)</span>
<span class="gp">#</span>SBATCH --ntasks<span class="o">=</span><span class="m">3</span>

<span class="gp"># </span>Number of GPU per node
<span class="gp">#</span>SBATCH --gres<span class="o">=</span>gpu:4
<span class="gp">#</span>SBATCH --gpus-per-node<span class="o">=</span><span class="m">4</span>

<span class="gp"># </span><span class="m">16</span> CPUs per node
<span class="gp">#</span>SBATCH --cpus-per-gpu<span class="o">=</span><span class="m">4</span>

<span class="gp"># </span>16Go per nodes <span class="o">(</span>4Go per GPU<span class="o">)</span>
<span class="gp">#</span>SBATCH --mem<span class="o">=</span>16G

<span class="gp"># </span>we need all nodes to be ready at the same <span class="nb">time</span>
<span class="gp">#</span>SBATCH --wait-all-nodes<span class="o">=</span><span class="m">1</span>

<span class="gp"># </span>Total resources:
<span class="gp">#   </span>CPU: <span class="m">16</span> * <span class="nv">3</span> <span class="o">=</span> <span class="m">48</span>
<span class="gp">#   </span>RAM: <span class="m">16</span> * <span class="nv">3</span> <span class="o">=</span> <span class="m">48</span> Go
<span class="gp">#   </span>GPU:  <span class="m">4</span> * <span class="nv">3</span> <span class="o">=</span> <span class="m">12</span>

<span class="gp"># </span>Setup our rendez-vous point
<span class="go">RDV_ADDR=$(hostname)</span>
<span class="go">WORLD_SIZE=$SLURM_JOB_NUM_NODES</span>
<span class="gp"># </span>-----

<span class="go">srun -l torchrun \</span>
<span class="go">    --nproc_per_node=$SLURM_GPUS_PER_NODE\</span>
<span class="go">    --nnodes=$WORLD_SIZE\</span>
<span class="go">    --rdzv_id=$SLURM_JOB_ID\</span>
<span class="go">    --rdzv_backend=c10d\</span>
<span class="go">    --rdzv_endpoint=$RDV_ADDR\</span>
<span class="go">    training_script.py</span>
</pre></div>
</div>
<p>You can find below a pytorch script outline on what a multi-node trainer could look like.</p>
<div class="highlight-python notranslate" id="training-script-outline-for-multi-node-training"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="k">class</span> <span class="nc">Trainer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chk_path</span> <span class="o">=</span> <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="o">...</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">device_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span>

    <span class="k">def</span> <span class="nf">load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chk_path</span> <span class="o">=</span> <span class="n">path</span>
        <span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">should_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Note: only one worker saves its weights</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">chk_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Save your states here</span>
        <span class="c1"># Note: you should save the weights of self.model not ddp_model</span>
        <span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;RANK&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Global rank should be set (Only Rank 0 can save checkpoints)&#39;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Local rank should be set&#39;</span>

        <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;gloo|nccl&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sync_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">resuming</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">resuming</span><span class="p">:</span>
            <span class="c1"># in the case of resuming all workers need to load the same checkpoint</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">()</span>

            <span class="c1"># Wait for everybody to finish loading the checkpoint</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
            <span class="k">return</span>

        <span class="c1"># Make sure all workers have the same initial weights</span>
        <span class="c1"># This makes the leader save his weights</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_checkpoint</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">()</span>

        <span class="c1"># All workers wait for the leader to finish</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

        <span class="c1"># All followers load the leader&#39;s weights</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_checkpoint</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">()</span>

        <span class="c1"># Leader waits for the follower to load the weights</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">ElasticDistributedSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">train_loader</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Your batch processing step here</span>
        <span class="c1"># ...</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">()</span>

        <span class="n">ddp_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">device_id</span><span class="p">],</span>
            <span class="n">output_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device_id</span>
        <span class="p">)</span>

        <span class="n">loader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">iter</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_checkpoint</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">tainer</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>To bypass Python GIL (Global interpreter lock) pytorch spawn one process for each GPU.
In the example above this means at least 12 processes are spawn, at least 4 on each node.</p>
</div>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/cin-logo.png" alt="Logo"/>
            </a></p>
  <div>
    <h3><a href="index.html">Tabela de Conteúdo</a></h3>
    <ul>
<li><a class="reference internal" href="#">Multiple Nodes</a><ul>
<li><a class="reference internal" href="#data-parallel">Data Parallel</a></li>
</ul>
</li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>Essa Página</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/Userguide_multinode.rst.txt"
            rel="nofollow">Exibir Fonte</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Busca rápida</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Ir" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navegação</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="Índice Geral"
             >índice</a></li>
        <li class="nav-item nav-item-0"><a href="index.html">documentação Cluster Cin latest</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Multiple Nodes</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2022.
      Criada usando <a href="https://www.sphinx-doc.org/pt_BR/master">Sphinx</a> 5.3.0.
    </div>
  </body>
</html>