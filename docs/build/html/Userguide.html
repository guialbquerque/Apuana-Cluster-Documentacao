
<!DOCTYPE html>

<html lang="pt-BR">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>User’s guide &#8212; documentação Cluster Cin latest</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/groundwork.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/translations.js"></script>
    <link rel="index" title="Índice" href="genindex.html" />
    <link rel="search" title="Buscar" href="search.html" />
    <link rel="prev" title="Computing infrastructure and policies" href="information.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navegação</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="Índice Geral"
             accesskey="I">índice</a></li>
        <li class="right" >
          <a href="information.html" title="Computing infrastructure and policies"
             accesskey="P">anterior</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">documentação Cluster Cin latest</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">User’s guide</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="user-s-guide">
<span id="userguide"></span><h1>User’s guide<a class="headerlink" href="#user-s-guide" title="Link permanente para este cabeçalho">¶</a></h1>
<p>…or <em>IDT’s list of opinionated howtos</em></p>
<p>This section seeks to provide users of the Apuana infrastructure with practical
knowledge, tips and tricks and example commands.</p>
<section id="running-your-code">
<h2>Running your code<a class="headerlink" href="#running-your-code" title="Link permanente para este cabeçalho">¶</a></h2>
<section id="slurm-commands-guide">
<h3>SLURM commands guide<a class="headerlink" href="#slurm-commands-guide" title="Link permanente para este cabeçalho">¶</a></h3>
<section id="basic-usage">
<h4>Basic Usage<a class="headerlink" href="#basic-usage" title="Link permanente para este cabeçalho">¶</a></h4>
<p>The SLURM <a class="reference external" href="https://slurm.schedmd.com/documentation.html">documentation</a>
provides extensive information on the available commands to query the cluster
status or submit jobs.</p>
<p>Below are some basic examples of how to use SLURM.</p>
</section>
<section id="submitting-jobs">
<h4>Submitting jobs<a class="headerlink" href="#submitting-jobs" title="Link permanente para este cabeçalho">¶</a></h4>
<section id="batch-job">
<h5>Batch job<a class="headerlink" href="#batch-job" title="Link permanente para este cabeçalho">¶</a></h5>
<p>In order to submit a batch job, you have to create a script containing the main
command(s) you would like to execute on the allocated resources/nodes.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>!/bin/bash
<span class="gp">#</span>SBATCH --job-name<span class="o">=</span><span class="nb">test</span>
<span class="gp">#</span>SBATCH --output<span class="o">=</span>job_output.txt
<span class="gp">#</span>SBATCH --error<span class="o">=</span>job_error.txt
<span class="gp">#</span>SBATCH --ntasks<span class="o">=</span><span class="m">1</span>
<span class="gp">#</span>SBATCH --time<span class="o">=</span><span class="m">10</span>:00
<span class="gp">#</span>SBATCH --mem<span class="o">=</span>100Gb

<span class="go">module load python/3.5</span>
<span class="go">python my_script.py</span>
</pre></div>
</div>
<p>Your job script is then submitted to SLURM with <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> (<a class="reference external" href="https://slurm.schedmd.com/sbatch.html">ref.</a>)</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sbatch job_script
<span class="go">sbatch: Submitted batch job 4323674</span>
</pre></div>
</div>
<p>The <em>working directory</em> of the job will be the one where your executed <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>.</p>
<div class="admonition tip">
<p class="admonition-title">Dica</p>
<p>Slurm directives can be specified on the command line alongside <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> or
inside the job script with a line starting with <code class="docutils literal notranslate"><span class="pre">#SBATCH</span></code>.</p>
</div>
</section>
<section id="interactive-job">
<h5>Interactive job<a class="headerlink" href="#interactive-job" title="Link permanente para este cabeçalho">¶</a></h5>
<p>Workload managers usually run batch jobs to avoid having to watch its
progression and let the scheduler run it as soon as resources are available. If
you want to get access to a shell while leveraging cluster resources, you can
submit an interactive jobs where the main executable is a shell with the
<code class="docutils literal notranslate"><span class="pre">srun/salloc</span></code> (<a class="reference external" href="https://slurm.schedmd.com/srun.html">srun</a>/<a class="reference external" href="https://slurm.schedmd.com/salloc.html">salloc</a>) commands</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">salloc</span>
</pre></div>
</div>
<p>Will start an interactive job on the first node available with the default
resources set in SLURM (1 task/1 CPU).  <code class="docutils literal notranslate"><span class="pre">srun</span></code> accepts the same arguments as
<code class="docutils literal notranslate"><span class="pre">sbatch</span></code> with the exception that the environment is not passed.</p>
<div class="admonition tip">
<p class="admonition-title">Dica</p>
<p>To pass your current environment to an interactive job, add
<code class="docutils literal notranslate"><span class="pre">--preserve-env</span></code> to <code class="docutils literal notranslate"><span class="pre">srun</span></code>.</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">salloc</span></code> can also be used and is mostly a wrapper around <code class="docutils literal notranslate"><span class="pre">srun</span></code> if provided
without more info but it gives more flexibility if for example you want to get
an allocation on multiple nodes.</p>
</section>
</section>
<section id="job-submission-arguments">
<h4>Job submission arguments<a class="headerlink" href="#job-submission-arguments" title="Link permanente para este cabeçalho">¶</a></h4>
<p>In order to accurately select the resources for your job, several arguments are
available. The most important ones are:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 26%" />
<col style="width: 74%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Argument</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>-n, –ntasks=&lt;number&gt;</p></td>
<td><p>The number of task in your script, usually =1</p></td>
</tr>
<tr class="row-odd"><td><p>-c, –cpus-per-task=&lt;ncpus&gt;</p></td>
<td><p>The number of cores for each task</p></td>
</tr>
<tr class="row-even"><td><p>-t, –time=&lt;time&gt;</p></td>
<td><p>Time requested for your job</p></td>
</tr>
<tr class="row-odd"><td><p>–mem=&lt;size[units]&gt;</p></td>
<td><p>Memory requested for all your tasks</p></td>
</tr>
<tr class="row-even"><td><p>–gres=&lt;list&gt;</p></td>
<td><p>Select generic resources such as GPUs for your job: <code class="docutils literal notranslate"><span class="pre">--gres=gpu:GPU_MODEL</span></code></p></td>
</tr>
</tbody>
</table>
<div class="admonition tip">
<p class="admonition-title">Dica</p>
<p>Always consider requesting the adequate amount of resources to improve the
scheduling of your job (small jobs always run first).</p>
</div>
</section>
<section id="checking-job-status">
<h4>Checking job status<a class="headerlink" href="#checking-job-status" title="Link permanente para este cabeçalho">¶</a></h4>
<p>To display <em>jobs</em> currently in queue, use <code class="docutils literal notranslate"><span class="pre">squeue</span></code> and to get only your jobs type</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>squeue -u <span class="nv">$USER</span>
<span class="go">JOBID   USER          NAME    ST  START_TIME         TIME NODES CPUS TRES_PER_NMIN_MEM NODELIST (REASON) COMMENT</span>
<span class="go">133     my_username   myjob   R   2019-03-28T18:33   0:50     1    2        N/A  7000M node1 (None) (null)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>The maximum number of jobs able to be submitted to the system per user is 1000 (MaxSubmitJobs=1000)
at any given time from the given association. If this limit is reached, new submission requests
will be denied until existing jobs in this association complete.</p>
</div>
</section>
<section id="removing-a-job">
<h4>Removing a job<a class="headerlink" href="#removing-a-job" title="Link permanente para este cabeçalho">¶</a></h4>
<p>To cancel your job simply use <code class="docutils literal notranslate"><span class="pre">scancel</span></code></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">scancel 4323674</span>
</pre></div>
</div>
</section>
</section>
<section id="partitioning">
<h3>Partitioning<a class="headerlink" href="#partitioning" title="Link permanente para este cabeçalho">¶</a></h3>
<p>Since we don’t have many GPUs on the cluster, resources must be shared as fairly
as possible.  The <code class="docutils literal notranslate"><span class="pre">--partition=/-p</span></code> flag of SLURM allows you to set the
priority you need for a job.  Each job assigned with a priority can preempt jobs
with a lower priority: <code class="docutils literal notranslate"><span class="pre">unkillable</span> <span class="pre">&gt;</span> <span class="pre">main</span> <span class="pre">&gt;</span> <span class="pre">long</span></code>. Once preempted, your job is
killed without notice and is automatically re-queued on the same partition until
resources are available. (To leverage a different preemption mechanism, see the
<a class="reference internal" href="#advanced-preemption"><span class="std std-ref">Handling preemption</span></a>)</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 33%" />
<col style="width: 30%" />
<col style="width: 14%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Flag</p></th>
<th class="head"><p>Max Resource Usage</p></th>
<th class="head"><p>Max Time</p></th>
<th class="head"><p>Note</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>--partition=unkillable</p></td>
<td><p>6  CPUs, mem=32G,  1 GPU</p></td>
<td><p>2 days</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>--partition=unkillable-cpu</p></td>
<td><p>2  CPUs, mem=16G</p></td>
<td><p>2 days</p></td>
<td><p>CPU-only jobs</p></td>
</tr>
<tr class="row-even"><td><p>--partition=short-unkillable</p></td>
<td><p>24 CPUs, mem=128G, 4 GPUs</p></td>
<td><p>3 hours (!)</p></td>
<td><p>Large but short jobs</p></td>
</tr>
<tr class="row-odd"><td><p>--partition=main</p></td>
<td><p>8  CPUs, mem=48G,  2 GPUs</p></td>
<td><p>5 days</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>--partition=main-cpu</p></td>
<td><p>8  CPUs, mem=64G</p></td>
<td><p>5 days</p></td>
<td><p>CPU-only jobs</p></td>
</tr>
<tr class="row-odd"><td><p>--partition=long</p></td>
<td><p>no limit of resources</p></td>
<td><p>7 days</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>--partition=long-cpu</p></td>
<td><p>no limit of resources</p></td>
<td><p>7 days</p></td>
<td><p>CPU-only jobs</p></td>
</tr>
</tbody>
</table>
<div class="admonition warning">
<p class="admonition-title">Aviso</p>
<blockquote>
<div><p>Historically, before the 2022 introduction of CPU-only nodes (e.g. the <code class="docutils literal notranslate"><span class="pre">cn-f</span></code>
series), CPU jobs ran side-by-side with the GPU jobs on GPU nodes. To prevent
them obstructing any GPU job, they were always lowest-priority and preemptible.
This was implemented by automatically assigning them to one of the now-obsolete
partitions <code class="docutils literal notranslate"><span class="pre">cpu_jobs</span></code>, <code class="docutils literal notranslate"><span class="pre">cpu_jobs_low</span></code> or <code class="docutils literal notranslate"><span class="pre">cpu_jobs_low-grace</span></code>.</p>
</div></blockquote>
<dl>
<dt><strong>Do not use these partition names anymore</strong>. Prefer the <code class="docutils literal notranslate"><span class="pre">*-cpu</span></code> partition</dt><dd><p>names defined above.</p>
<p>For backwards-compatibility purposes, the legacy partition names are translated
to their effective equivalent <code class="docutils literal notranslate"><span class="pre">long-cpu</span></code>, but they will eventually be removed
entirely.</p>
</dd>
</dl>
</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<dl class="simple">
<dt><em>As a convenience</em>, should you request the <code class="docutils literal notranslate"><span class="pre">unkillable</span></code>, <code class="docutils literal notranslate"><span class="pre">main</span></code> or <code class="docutils literal notranslate"><span class="pre">long</span></code></dt><dd><p>partition for a CPU-only job, the partition will be translated to its <code class="docutils literal notranslate"><span class="pre">-cpu</span></code>
equivalent automatically.</p>
</dd>
</dl>
</div>
<p>For instance, to request an unkillable job with 1 GPU, 4 CPUs, 10G of RAM and
12h of computation do:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">sbatch --gres=gpu:1 -c 4 --mem=10G -t 12:00:00 --partition=unkillable &lt;job.sh&gt;</span>
</pre></div>
</div>
<p>You can also make it an interactive job using <code class="docutils literal notranslate"><span class="pre">salloc</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">salloc --gres=gpu:1 -c 4 --mem=10G -t 12:00:00 --partition=unkillable</span>
</pre></div>
</div>
<p>The Mila cluster has many different types of nodes/GPUs. To request a specific
type of node/GPU, you can add specific feature requirements to your job
submission command.</p>
<p>To access those special nodes you need to request them explicitly by adding the
flag <code class="docutils literal notranslate"><span class="pre">--constraint=&lt;name&gt;</span></code>.  The full list of nodes in the Mila Cluster can be
accessed <span class="xref std std-ref">Node profile description</span>.</p>
<p><em>Example:</em></p>
<p>To request a machine with 2 GPUs using NVLink, you can use</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">sbatch -c 4 --gres=gpu:2 --constraint=nvlink</span>
</pre></div>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 37%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Particularities</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>12GB/16GB/24GB/32GB/48GB</p></td>
<td><p>Request a specific amount of <em>GPU</em> memory</p></td>
</tr>
<tr class="row-odd"><td><p>volta/turing/ampere</p></td>
<td><p>Request a specific <em>GPU</em> architecture</p></td>
</tr>
<tr class="row-even"><td><p>nvlink</p></td>
<td><p>Machine with GPUs using the NVLink interconnect technology</p></td>
</tr>
</tbody>
</table>
<section id="information-on-partitions-nodes">
<h4>Information on partitions/nodes<a class="headerlink" href="#information-on-partitions-nodes" title="Link permanente para este cabeçalho">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">sinfo</span></code> (<a class="reference external" href="https://slurm.schedmd.com/sinfo.html">ref.</a>) provides most of the
information about available nodes and partitions/queues to submit jobs to.</p>
<p>Partitions are a group of nodes usually sharing similar features. On a
partition, some job limits can be applied which will override those asked for a
job (i.e. max time, max CPUs, etc…)</p>
<p>To display available <em>partitions</em>, simply use</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sinfo
<span class="go">PARTITION AVAIL TIMELIMIT NODES STATE  NODELIST</span>
<span class="go">batch     up     infinite     2 alloc  node[1,3,5-9]</span>
<span class="go">batch     up     infinite     6 idle   node[10-15]</span>
<span class="go">cpu       up     infinite     6 idle   cpu_node[1-15]</span>
<span class="go">gpu       up     infinite     6 idle   gpu_node[1-15]</span>
</pre></div>
</div>
<p>To display available <em>nodes</em> and their status, you can use</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sinfo -N -l
<span class="go">NODELIST    NODES PARTITION STATE  CPUS MEMORY TMP_DISK WEIGHT FEATURES REASON</span>
<span class="go">node[1,3,5-9]   2 batch     allocated 2    246    16000     0  (null)   (null)</span>
<span class="go">node[2,4]       2 batch     drain     2    246    16000     0  (null)   (null)</span>
<span class="go">node[10-15]     6 batch     idle      2    246    16000     0  (null)   (null)</span>
<span class="go">...</span>
</pre></div>
</div>
<p>And to get statistics on a job running or terminated, use <code class="docutils literal notranslate"><span class="pre">sacct</span></code> with some of
the fields you want to display</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>sacct --format<span class="o">=</span>User,JobID,Jobname,partition,state,time,start,end,elapsed,nnodes,ncpus,nodelist,workdir -u <span class="nv">$USER</span>
<span class="go">    User        JobID    JobName  Partition      State  Timelimit               Start                 End    Elapsed   NNodes      NCPUS        NodeList              WorkDir</span>
<span class="go">--------- ------------ ---------- ---------- ---------- ---------- ------------------- ------------------- ---------- -------- ---------- --------------- --------------------</span>
<span class="go">my_usern+ 2398         run_extra+      batch    RUNNING 130-05:00+ 2019-03-27T18:33:43             Unknown 1-01:07:54        1         16 node9           /home/mila/my_usern+</span>
<span class="go">my_usern+ 2399         run_extra+      batch    RUNNING 130-05:00+ 2019-03-26T08:51:38             Unknown 2-10:49:59        1         16 node9           /home/mila/my_usern+</span>
</pre></div>
</div>
<p>Or to get the list of all your previous jobs, use the <code class="docutils literal notranslate"><span class="pre">--start=YYYY-MM-DD</span></code> flag. You can check <code class="docutils literal notranslate"><span class="pre">sacct(1)</span></code> for further information about additional time formats.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">sacct -u $USER --start=2019-01-01</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">scontrol</span></code> (<a class="reference external" href="https://slurm.schedmd.com/scontrol.html">ref.</a>) can be used to
provide specific information on a job (currently running or recently terminated)</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>scontrol show job <span class="m">43123</span>
<span class="go">JobId=43123 JobName=python_script.py</span>
<span class="go">UserId=my_username(1500000111) GroupId=student(1500000000) MCS_label=N/A</span>
<span class="go">Priority=645895 Nice=0 Account=my_username QOS=normal</span>
<span class="go">JobState=RUNNING Reason=None Dependency=(null)</span>
<span class="go">Requeue=1 Restarts=3 BatchFlag=1 Reboot=0 ExitCode=0:0</span>
<span class="go">RunTime=2-10:41:57 TimeLimit=130-05:00:00 TimeMin=N/A</span>
<span class="go">SubmitTime=2019-03-26T08:47:17 EligibleTime=2019-03-26T08:49:18</span>
<span class="go">AccrueTime=2019-03-26T08:49:18</span>
<span class="go">StartTime=2019-03-26T08:51:38 EndTime=2019-08-03T13:51:38 Deadline=N/A</span>
<span class="go">PreemptTime=None SuspendTime=None SecsPreSuspend=0</span>
<span class="go">LastSchedEval=2019-03-26T08:49:18</span>
<span class="go">Partition=slurm_partition AllocNode:Sid=login-node-1:14586</span>
<span class="go">ReqNodeList=(null) ExcNodeList=(null)</span>
<span class="go">NodeList=node2</span>
<span class="go">BatchHost=node2</span>
<span class="go">NumNodes=1 NumCPUs=16 NumTasks=1 CPUs/Task=16 ReqB:S:C:T=0:0:*:*</span>
<span class="go">TRES=cpu=16,mem=32000M,node=1,billing=3</span>
<span class="go">Socks/Node=* NtasksPerN:B:S:C=1:0:*:* CoreSpec=*</span>
<span class="go">MinCPUsNode=16 MinMemoryNode=32000M MinTmpDiskNode=0</span>
<span class="go">Features=(null) DelayBoot=00:00:00</span>
<span class="go">OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)</span>
<span class="go">WorkDir=/home/mila/my_username</span>
<span class="go">StdErr=/home/mila/my_username/slurm-43123.out</span>
<span class="go">StdIn=/dev/null</span>
<span class="go">StdOut=/home/mila/my_username/slurm-43123.out</span>
<span class="go">Power=</span>
</pre></div>
</div>
<p>Or more info on a node and its resources</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>scontrol show node node9
<span class="go">NodeName=node9 Arch=x86_64 CoresPerSocket=4</span>
<span class="go">CPUAlloc=16 CPUTot=16 CPULoad=1.38</span>
<span class="go">AvailableFeatures=(null)</span>
<span class="go">ActiveFeatures=(null)</span>
<span class="go">Gres=(null)</span>
<span class="go">NodeAddr=10.252.232.4 NodeHostName=mila20684000000 Port=0 Version=18.08</span>
<span class="go">OS=Linux 4.15.0-1036 #38-Ubuntu SMP Fri Dec 7 02:47:47 UTC 2018</span>
<span class="go">RealMemory=32000 AllocMem=32000 FreeMem=23262 Sockets=2 Boards=1</span>
<span class="go">State=ALLOCATED+CLOUD ThreadsPerCore=2 TmpDisk=0 Weight=1 Owner=N/A MCS_label=N/A</span>
<span class="go">Partitions=slurm_partition</span>
<span class="go">BootTime=2019-03-26T08:50:01 SlurmdStartTime=2019-03-26T08:51:15</span>
<span class="go">CfgTRES=cpu=16,mem=32000M,billing=3</span>
<span class="go">AllocTRES=cpu=16,mem=32000M</span>
<span class="go">CapWatts=n/a</span>
<span class="go">CurrentWatts=0 LowestJoules=0 ConsumedJoules=0</span>
<span class="go">ExtSensorsJoules=n/s ExtSensorsWatts=0 ExtSensorsTemp=n/s</span>
</pre></div>
</div>
</section>
</section>
<section id="useful-commands">
<h3>Useful Commands<a class="headerlink" href="#useful-commands" title="Link permanente para este cabeçalho">¶</a></h3>
<dl class="simple">
<dt>salloc</dt><dd><p>Get an interactive job and give you a shell. (ssh like) CPU only</p>
</dd>
<dt>salloc --gres=gpu:1 -c 2 --mem=12000</dt><dd><p>Get an interactive job with one GPU, 2 CPUs and 12000 MB RAM</p>
</dd>
<dt>sbatch</dt><dd><p>start a batch job (same options as salloc)</p>
</dd>
<dt>sattach --pty &lt;jobid&gt;.0</dt><dd><p>Re-attach a dropped interactive job</p>
</dd>
<dt>sinfo</dt><dd><p>status of all nodes</p>
</dd>
<dt>sinfo -Ogres:27,nodelist,features -tidle,mix,alloc</dt><dd><p>List GPU type and FEATURES that you can request</p>
</dd>
<dt>savail</dt><dd><p>(Custom) List available gpu</p>
</dd>
<dt>scancel &lt;jobid&gt;</dt><dd><p>Cancel a job</p>
</dd>
<dt>squeue</dt><dd><p>summary status of all active jobs</p>
</dd>
<dt>squeue -u $USER</dt><dd><p>summary status of all YOUR active jobs</p>
</dd>
<dt>squeue -j &lt;jobid&gt;</dt><dd><p>summary status of a specific job</p>
</dd>
<dt>squeue -Ojobid,name,username,partition,state,timeused,nodelist,gres,tres</dt><dd><p>status of all jobs including requested resources (see the SLURM squeue doc for all output options)</p>
</dd>
<dt>scontrol show job &lt;jobid&gt;</dt><dd><p>Detailed status of a running job</p>
</dd>
<dt>sacct -j &lt;job_id&gt; -o NodeList</dt><dd><p>Get the node where a finished job ran</p>
</dd>
<dt>sacct -u $USER -S &lt;start_time&gt; -E &lt;stop_time&gt;</dt><dd><p>Find info about old jobs</p>
</dd>
<dt>sacct -oJobID,JobName,User,Partition,Node,State</dt><dd><p>List of current and recent jobs</p>
</dd>
</dl>
</section>
<section id="special-gpu-requirements">
<h3>Special GPU requirements<a class="headerlink" href="#special-gpu-requirements" title="Link permanente para este cabeçalho">¶</a></h3>
<p>Specific GPU <em>architecture</em> and <em>memory</em> can be easily requested through the
<code class="docutils literal notranslate"><span class="pre">--gres</span></code> flag by using either</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--gres=gpu:architecture:number</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gres=gpu:memory:number</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--gres=gpu:model:number</span></code></p></li>
</ul>
<p><em>Example:</em></p>
<p>To request 1 GPU with <em>at least</em> 16GB of memory use</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch -c <span class="m">4</span> --gres<span class="o">=</span>gpu:16gb:1
</pre></div>
</div>
<p>The full list of GPU and their features can be accessed <span class="xref std std-ref">here</span>.</p>
</section>
<section id="example-script">
<h3>Example script<a class="headerlink" href="#example-script" title="Link permanente para este cabeçalho">¶</a></h3>
<p>Here is a <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> script that follows good practices on the Mila cluster:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>

<span class="c1">#SBATCH --partition=unkillable                           # Ask for unkillable job</span>
<span class="c1">#SBATCH --cpus-per-task=2                                # Ask for 2 CPUs</span>
<span class="c1">#SBATCH --gres=gpu:1                                     # Ask for 1 GPU</span>
<span class="c1">#SBATCH --mem=10G                                        # Ask for 10 GB of RAM</span>
<span class="c1">#SBATCH --time=3:00:00                                   # The job will run for 3 hours</span>
<span class="c1">#SBATCH -o /network/scratch/&lt;u&gt;/&lt;username&gt;/slurm-%j.out  # Write the log on scratch</span>

<span class="c1"># 1. Load the required modules</span>
module --quiet load anaconda/3

<span class="c1"># 2. Load your environment</span>
conda activate <span class="s2">&quot;&lt;env_name&gt;&quot;</span>

<span class="c1"># 3. Copy your dataset on the compute node</span>
cp /network/datasets/&lt;dataset&gt; <span class="nv">$SLURM_TMPDIR</span>

<span class="c1"># 4. Launch your job, tell it to save the model in $SLURM_TMPDIR</span>
<span class="c1">#    and look for the dataset into $SLURM_TMPDIR</span>
python main.py --path <span class="nv">$SLURM_TMPDIR</span> --data_path <span class="nv">$SLURM_TMPDIR</span>

<span class="c1"># 5. Copy whatever you want to save on $SCRATCH</span>
cp <span class="nv">$SLURM_TMPDIR</span>/&lt;to_save&gt; /network/scratch/&lt;u&gt;/&lt;username&gt;/
</pre></div>
</div>
</section>
</section>
<section id="portability-concerns-and-solutions">
<h2>Portability concerns and solutions<a class="headerlink" href="#portability-concerns-and-solutions" title="Link permanente para este cabeçalho">¶</a></h2>
<p>When working on a software project, it is important to be aware of all the
software and libraries the project relies on and to list them explicitly and
<em>under a version control system</em> in such a way that they can easily be
installed and made available on different systems. The upsides are significant:</p>
<ul class="simple">
<li><p>Easily install and run on the cluster</p></li>
<li><p>Ease of collaboration</p></li>
<li><p>Better reproducibility</p></li>
</ul>
<p>To achieve this, try to always keep in mind the following aspects:</p>
<ul class="simple">
<li><dl class="simple">
<dt><strong>Versions:</strong> For each dependency, make sure you have some record of the</dt><dd><p>specific version you are using during development. That way, in the future, you
will be able to reproduce the original environment which you know to be
compatible. Indeed, the more time passes, the more likely it is that newer
versions of some dependency have breaking changes. The <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">freeze</span></code> command can create
such a record for Python dependencies.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Isolation:</strong> Ideally, each of your software projects should be isolated from</dt><dd><p>the others. What this means is that updating the environment for project A</p>
</dd>
<dt>should <em>not</em> update the environment for project B. That way, you can freely</dt><dd><p>install and upgrade software and libraries for the former without worrying about
breaking the latter (which you might not notice until weeks later, the next time
you work on project B!) Isolation can be achieved using <span class="xref std std-ref">Python Virtual
environments</span> and <span class="xref std std-ref">containers</span>.</p>
</dd>
</dl>
</li>
</ul>
<section id="managing-your-environments">
<h3>Managing your environments<a class="headerlink" href="#managing-your-environments" title="Link permanente para este cabeçalho">¶</a></h3>
</section>
<section id="virtual-environments">
<span id="python"></span><h3>Virtual environments<a class="headerlink" href="#virtual-environments" title="Link permanente para este cabeçalho">¶</a></h3>
<p>A virtual environment in Python is a local, isolated environment in which you
can install or uninstall Python packages without interfering with the global
environment (or other virtual environments). It usually lives in a directory
(location varies depending on whether you use venv, conda or poetry). In order
to use a virtual environment, you have to <strong>activate</strong> it. Activating an
environment essentially sets environment variables in your shell so that:</p>
<ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">python</span></code> points to the right Python version for that environment (different</dt><dd><p>virtual environments can use different versions of Python!)</p>
</dd>
</dl>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">python</span></code> looks for packages in the virtual environment</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code> installs packages into the virtual environment</p></li>
<li><p>Any shell commands installed via <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span></code> are made available</p></li>
</ul>
<p>To run experiments within a virtual environment, you can simply activate it
in the script given to <code class="docutils literal notranslate"><span class="pre">sbatch</span></code>.</p>
<section id="pip-virtualenv">
<h4>Pip/Virtualenv<a class="headerlink" href="#pip-virtualenv" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Pip is the preferred package manager for Python and each cluster provides
several Python versions through the associated module which comes with pip. In
order to install new packages, you will first have to create a personal space
for them to be stored.  The preferred solution (as it is the preferred solution
on Digital Research Alliance of Canada clusters) is to use <a class="reference external" href="https://virtualenv.pypa.io/en/stable/">virtual
environments</a>.</p>
<p>First, load the Python module you want to use:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">module load python/3.8</span>
</pre></div>
</div>
<p>Then, create a virtual environment in your <code class="docutils literal notranslate"><span class="pre">home</span></code> directory:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">python -m venv $HOME/&lt;env&gt;</span>
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">&lt;env&gt;</span></code> is the name of your environment. Finally, activate the environment:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">source $HOME/&lt;env&gt;/bin/activate</span>
</pre></div>
</div>
<p>You can now install any Python package you wish using the <code class="docutils literal notranslate"><span class="pre">pip</span></code> command, e.g.
<a class="reference external" href="https://pytorch.org/get-started/locally">pytorch</a>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">pip install torch torchvision</span>
</pre></div>
</div>
<p>Or <a class="reference external" href="https://www.tensorflow.org/install/gpu">Tensorflow</a>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">pip install tensorflow-gpu</span>
</pre></div>
</div>
</section>
<section id="conda">
<h4>Conda<a class="headerlink" href="#conda" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Another solution for Python is to use <a class="reference external" href="https://docs.conda.io/en/latest/miniconda.html">miniconda</a> or <a class="reference external" href="https://docs.anaconda.com">anaconda</a> which are also available through the <code class="docutils literal notranslate"><span class="pre">module</span></code>
command: (the use of Conda is not recommended for Digital Research Alliance of
Canada clusters due to the availability of custom-built packages for pip)</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>module load miniconda/3
<span class="go">[=== Module miniconda/3 loaded ===]</span>
<span class="go">To enable conda environment functions, first use:</span>
</pre></div>
</div>
<p>To create an environment (see <a class="reference external" href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html">here</a>
for details) using a specific Python version, you may write:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">conda create -n &lt;env&gt; python=3.9</span>
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">&lt;env&gt;</span></code> is the name of your environment. You can now activate it by doing:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">conda activate &lt;env&gt;</span>
</pre></div>
</div>
<p>You are now ready to install any Python package you want in this environment.
For instance, to install PyTorch, you can find the Conda command of any version
you want on <a class="reference external" href="https://pytorch.org/get-started/locally">pytorch’s website</a>, e.g:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">conda install pytorch torchvision cudatoolkit=10.0 -c pytorch</span>
</pre></div>
</div>
<p>If you make a lot of environments and install/uninstall a lot of packages, it
can be good to periodically clean up Conda’s cache:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">conda clean --all</span>
</pre></div>
</div>
</section>
</section>
<section id="using-modules">
<h3>Using Modules<a class="headerlink" href="#using-modules" title="Link permanente para este cabeçalho">¶</a></h3>
<p>A lot of software, such as Python and Conda, is already compiled and available on
the cluster through the <code class="docutils literal notranslate"><span class="pre">module</span></code> command and its sub-commands. In particular,
if you wish to use <code class="docutils literal notranslate"><span class="pre">Python</span> <span class="pre">3.7</span></code> you can simply do:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">module load python/3.7</span>
</pre></div>
</div>
<section id="the-module-command">
<h4>The module command<a class="headerlink" href="#the-module-command" title="Link permanente para este cabeçalho">¶</a></h4>
<p>For a list of available modules, simply use:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>module avail
<span class="go">--------------------------------------------------------------------------------------------------------------- Global Aliases ---------------------------------------------------------------------------------------------------------------</span>
<span class="go">    cuda/10.0 -&gt; cudatoolkit/10.0    cuda/9.2      -&gt; cudatoolkit/9.2                                 pytorch/1.4.1       -&gt; python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.4.1    tensorflow/1.15 -&gt; python/3.7/tensorflow/1.15</span>
<span class="go">    cuda/10.1 -&gt; cudatoolkit/10.1    mujoco-py     -&gt; python/3.7/mujoco-py/2.0                        pytorch/1.5.0       -&gt; python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.0    tensorflow/2.2  -&gt; python/3.7/tensorflow/2.2</span>
<span class="go">    cuda/10.2 -&gt; cudatoolkit/10.2    mujoco-py/2.0 -&gt; python/3.7/mujoco-py/2.0                        pytorch/1.5.1       -&gt; python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.1</span>
<span class="go">    cuda/11.0 -&gt; cudatoolkit/11.0    pytorch       -&gt; python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.1    tensorflow          -&gt; python/3.7/tensorflow/2.2</span>
<span class="go">    cuda/9.0  -&gt; cudatoolkit/9.0     pytorch/1.4.0 -&gt; python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.4.0    tensorflow-cpu/1.15 -&gt; python/3.7/tensorflow/1.15</span>

<span class="go">--------------------------------------------------------------------------------------------------- /cvmfs/config.mila.quebec/modules/Core ---------------------------------------------------------------------------------------------------</span>
<span class="go">    Mila       (S,L)    anaconda/3 (D)    go/1.13.5        miniconda/2        mujoco/1.50        python/2.7    python/3.6        python/3.8           singularity/3.0.3    singularity/3.2.1    singularity/3.5.3 (D)</span>
<span class="go">    anaconda/2          go/1.12.4         go/1.14   (D)    miniconda/3 (D)    mujoco/2.0  (D)    python/3.5    python/3.7 (D)    singularity/2.6.1    singularity/3.1.1    singularity/3.4.2</span>

<span class="go">------------------------------------------------------------------------------------------------- /cvmfs/config.mila.quebec/modules/Compiler -------------------------------------------------------------------------------------------------</span>
<span class="go">    python/3.7/mujoco-py/2.0</span>

<span class="go">--------------------------------------------------------------------------------------------------- /cvmfs/config.mila.quebec/modules/Cuda ---------------------------------------------------------------------------------------------------</span>
<span class="go">    cuda/10.0/cudnn/7.3        cuda/10.0/nccl/2.4         cuda/10.1/nccl/2.4     cuda/11.0/nccl/2.7        cuda/9.0/nccl/2.4     cudatoolkit/9.0     cudatoolkit/10.1        cudnn/7.6/cuda/10.0/tensorrt/7.0</span>
<span class="go">    cuda/10.0/cudnn/7.5        cuda/10.1/cudnn/7.5        cuda/10.2/cudnn/7.6    cuda/9.0/cudnn/7.3        cuda/9.2/cudnn/7.6    cudatoolkit/9.2     cudatoolkit/10.2        cudnn/7.6/cuda/10.1/tensorrt/7.0</span>
<span class="go">    cuda/10.0/cudnn/7.6 (D)    cuda/10.1/cudnn/7.6 (D)    cuda/10.2/nccl/2.7     cuda/9.0/cudnn/7.5 (D)    cuda/9.2/nccl/2.4     cudatoolkit/10.0    cudatoolkit/11.0 (D)    cudnn/7.6/cuda/9.0/tensorrt/7.0</span>

<span class="go">------------------------------------------------------------------------------------------------- /cvmfs/config.mila.quebec/modules/Pytorch --------------------------------------------------------------------------------------------------</span>
<span class="go">    python/3.7/cuda/10.1/cudnn/7.6/pytorch/1.4.1    python/3.7/cuda/10.1/cudnn/7.6/pytorch/1.5.1 (D)    python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.0</span>
<span class="go">    python/3.7/cuda/10.1/cudnn/7.6/pytorch/1.5.0    python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.4.1        python/3.7/cuda/10.2/cudnn/7.6/pytorch/1.5.1 (D)</span>

<span class="go">------------------------------------------------------------------------------------------------ /cvmfs/config.mila.quebec/modules/Tensorflow ------------------------------------------------------------------------------------------------</span>
<span class="go">    python/3.7/tensorflow/1.15    python/3.7/tensorflow/2.0    python/3.7/tensorflow/2.2 (D)</span>
</pre></div>
</div>
<p>Modules can be loaded using the <code class="docutils literal notranslate"><span class="pre">load</span></code> command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">module load &lt;module&gt;</span>
</pre></div>
</div>
<p>To search for a module or a software, use the command <code class="docutils literal notranslate"><span class="pre">spider</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">module spider search_term</span>
</pre></div>
</div>
<p>E.g.: by default, <code class="docutils literal notranslate"><span class="pre">python2</span></code> will refer to the os-shipped installation of <code class="docutils literal notranslate"><span class="pre">python2.7</span></code> and <code class="docutils literal notranslate"><span class="pre">python3</span></code> to <code class="docutils literal notranslate"><span class="pre">python3.6</span></code>.
If you want to use <code class="docutils literal notranslate"><span class="pre">python3.7</span></code> you can type:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">module load python3.7</span>
</pre></div>
</div>
</section>
<section id="available-software">
<h4>Available Software<a class="headerlink" href="#available-software" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Modules are divided in 5 main sections:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 75%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Section</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Core</p></td>
<td><p>Base interpreter and software (Python, go, etc…)</p></td>
</tr>
<tr class="row-odd"><td><p>Compiler</p></td>
<td><p>Interpreter-dependent software (<em>see the note below</em>)</p></td>
</tr>
<tr class="row-even"><td><p>Cuda</p></td>
<td><p>Toolkits, cudnn and related libraries</p></td>
</tr>
<tr class="row-odd"><td><p>Pytorch/Tensorflow</p></td>
<td><p>Pytorch/TF built with a specific Cuda/Cudnn
version for Mila’s GPUs (<em>see the related paragraph</em>)</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>Modules which are nested (../../..) usually depend on other software/module
loaded alongside the main module.  No need to load the dependent software,
the complex naming scheme allows an automatic detection of the dependent
module(s):</p>
<p>i.e.: Loading <code class="docutils literal notranslate"><span class="pre">cudnn/7.6/cuda/9.0/tensorrt/7.0</span></code> will load <code class="docutils literal notranslate"><span class="pre">cudnn/7.6</span></code> and
<code class="docutils literal notranslate"><span class="pre">cuda/9.0</span></code> alongside</p>
<p><code class="docutils literal notranslate"><span class="pre">python/3.X</span></code> is a particular dependency which can be served through
<code class="docutils literal notranslate"><span class="pre">python/3.X</span></code> or <code class="docutils literal notranslate"><span class="pre">anaconda/3</span></code> and is not automatically loaded to let the
user pick his favorite flavor.</p>
</div>
</section>
<section id="default-package-location">
<h4>Default package location<a class="headerlink" href="#default-package-location" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Python by default uses the user site package first and packages provided by
<code class="docutils literal notranslate"><span class="pre">module</span></code> last to not interfere with your installation.  If you want to skip
packages installed in your site-packages folder (in your /home directory), you
have to start Python with the <code class="docutils literal notranslate"><span class="pre">-s</span></code> flag.</p>
<p>To check which package is loaded at import, you can print <code class="docutils literal notranslate"><span class="pre">package.__file__</span></code>
to get the full path of the package.</p>
<p><em>Example:</em></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>module load pytorch/1.5.0
<span class="gp">$ </span>python -c <span class="s1">&#39;import torch;print(torch.__file__)&#39;</span>
<span class="go">/home/mila/my_home/.local/lib/python3.7/site-packages/torch/__init__.py   &lt;== package from your own site-package</span>
</pre></div>
</div>
<p>Now with the <code class="docutils literal notranslate"><span class="pre">-s</span></code> flag:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>module load pytorch/1.5.0
<span class="gp">$ </span>python -s -c <span class="s1">&#39;import torch;print(torch.__file__)&#39;</span>
<span class="go">/cvmfs/ai.mila.quebec/apps/x86_64/debian/pytorch/python3.7-cuda10.1-cudnn7.6-v1.5.0/lib/python3.7/site-packages/torch/__init__.py&#39;</span>
</pre></div>
</div>
</section>
</section>
<section id="on-using-containers">
<h3>On using containers<a class="headerlink" href="#on-using-containers" title="Link permanente para este cabeçalho">¶</a></h3>
<p>Another option for creating portable code is <a class="reference internal" href="#using-containers"><span class="std std-ref">Using containers on clusters</span></a>.</p>
<p>Containers are a popular approach at deploying applications by packaging a lot
of the required dependencies together. The most popular tool for this is
<a class="reference external" href="https://www.docker.com/">Docker</a>, but Docker cannot be used on the Mila
cluster (nor the other clusters from Digital Research Alliance of Canada).</p>
<p>One popular mechanism for containerisation on a computational cluster is called
<a class="reference external" href="https://singularity-docs.readthedocs.io/en/latest/">Singularity</a>.
This is the recommended approach for running containers on the
Mila cluster. See section <span class="xref std std-ref">Singularity</span> for more details.</p>
</section>
</section>
<section id="id2">
<h2>Singularity<a class="headerlink" href="#id2" title="Link permanente para este cabeçalho">¶</a></h2>
<section id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Link permanente para este cabeçalho">¶</a></h3>
<section id="what-is-singularity">
<h4>What is Singularity?<a class="headerlink" href="#what-is-singularity" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Running Docker on SLURM is a security problem (e.g. running as root, being able
to mount any directory).  The alternative is to use Singularity, which is a
popular solution in the world of HPC.</p>
<p>There is a good level of compatibility between Docker and Singularity,
and we can find many exaggerated claims about able to convert containers
from Docker to Singularity without any friction.
Oftentimes, Docker images from DockerHub are 100% compatible with Singularity,
and they can indeed be used without friction, but things get messy when
we try to convert our own Docker build files to Singularity recipes.</p>
</section>
<section id="links-to-official-documentation">
<h4>Links to official documentation<a class="headerlink" href="#links-to-official-documentation" title="Link permanente para este cabeçalho">¶</a></h4>
<ul class="simple">
<li><p>official <a class="reference external" href="https://singularity-docs.readthedocs.io/en/latest/">Singularity user guide</a> (this is the one you will use most often)</p></li>
<li><p>official <a class="reference external" href="https://sylabs.io/guides/latest/admin-guide/">Singularity admin guide</a></p></li>
</ul>
</section>
<section id="overview-of-the-steps-used-in-practice">
<h4>Overview of the steps used in practice<a class="headerlink" href="#overview-of-the-steps-used-in-practice" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Most often, the process to create and use a Singularity container is:</p>
<ul class="simple">
<li><p>on your Linux computer (at home or work)</p>
<ul>
<li><p>select a Docker image from DockerHub (e.g. <em>pytorch/pytorch</em>)</p></li>
<li><p>make a recipe file for Singularity that starts with that DockerHub image</p></li>
<li><p>build the recipe file, thus creating the image file (e.g. <code class="docutils literal notranslate"><span class="pre">my-pytorch-image.sif</span></code>)</p></li>
<li><p>test your singularity container before send it over to the cluster</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rsync</span> <span class="pre">-av</span> <span class="pre">my-pytorch-image.sif</span> <span class="pre">&lt;login-node&gt;:Documents/my-singularity-images</span></code></p></li>
</ul>
</li>
<li><p>on the login node for that cluster</p>
<ul>
<li><p>queue your jobs with <code class="docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">...</span></code></p></li>
<li><p>(note that your jobs will copy over the <code class="docutils literal notranslate"><span class="pre">my-pytorch-image.sif</span></code> to $SLURM_TMPDIR
and will then launch Singularity with that image)</p></li>
<li><p>do something else while you wait for them to finish</p></li>
<li><p>queue more jobs with the same <code class="docutils literal notranslate"><span class="pre">my-pytorch-image.sif</span></code>,
reusing it many times over</p></li>
</ul>
</li>
</ul>
<p>In the following sections you will find specific examples or tips to accomplish
in practice the steps highlighted above.</p>
</section>
<section id="nope-not-on-macos">
<h4>Nope, not on MacOS<a class="headerlink" href="#nope-not-on-macos" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Singularity does not work on MacOS, as of the time of this writing in 2021.
Docker does not <em>actually</em> run on MacOS, but there Docker silently installs a
virtual machine running Linux, which makes it a pleasant experience,
and the user does not need to care about the details of how Docker does it.</p>
<p>Given its origins in HPC, Singularity does not provide that kind of seamless
experience on MacOS, even though it’s technically possible to run it
inside a Linux virtual machine on MacOS.</p>
</section>
<section id="where-to-build-images">
<h4>Where to build images<a class="headerlink" href="#where-to-build-images" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Building Singularity images is a rather heavy task, which can take 20 minutes
if you have a lot of steps in your recipe. This makes it a bad task to run on
the login nodes of our clusters, especially if it needs to be run regularly.</p>
<p>On the Mila cluster, we are lucky to have unrestricted internet access on the compute
nodes, which means that anyone can request an interactive CPU node (no need for GPU)
and build their images there without problem.</p>
<div class="admonition warning">
<p class="admonition-title">Aviso</p>
<p>Do not build Singularity images from scratch every time your run a
job in a large batch.  This will be a colossal waste of GPU time as well as
internet bandwidth.  If you setup your workflow properly (e.g. using bind
paths for your code and data), you can spend months reusing the same
Singularity image <code class="docutils literal notranslate"><span class="pre">my-pytorch-image.sif</span></code>.</p>
</div>
</section>
</section>
<section id="building-the-containers">
<h3>Building the containers<a class="headerlink" href="#building-the-containers" title="Link permanente para este cabeçalho">¶</a></h3>
<p>Building a container is like creating a new environment except that containers
are much more powerful since they are self-contained systems.  With
singularity, there are two ways to build containers.</p>
<p>The first one is by yourself, it’s like when you got a new Linux laptop and you
don’t really know what you need, if you see that something is missing, you
install it. Here you can get a vanilla container with Ubuntu called a sandbox,
you log in and you install each packages by yourself.  This procedure can take
time but will allow you to understand how things work and what you need. This is
recommended if you need to figure out how things will be compiled or if you want
to install packages on the fly. We’ll refer to this procedure as singularity
sandboxes.</p>
<p>The second way is more like you know what you want, so you write a list of
everything you need, you send it to singularity and it will install everything
for you. Those lists are called singularity recipes.</p>
<section id="first-way-build-and-use-a-sandbox">
<h4>First way: Build and use a sandbox<a class="headerlink" href="#first-way-build-and-use-a-sandbox" title="Link permanente para este cabeçalho">¶</a></h4>
<p>You might ask yourself: <em>On which machine should I build a container?</em></p>
<p>First of all, you need to choose where you’ll build your container. This
operation requires <strong>memory and high cpu usage</strong>.</p>
<div class="admonition warning">
<p class="admonition-title">Aviso</p>
<p>Do NOT build containers on any login nodes !</p>
</div>
<ul>
<li><dl>
<dt>(Recommended for beginner) If you need to <strong>use apt-get</strong>, you should <a href="#id3"><span class="problematic" id="id4">**</span></a>build</dt><dd><p>the container on your laptop** with sudo privileges. You’ll only need to
install singularity on your laptop. Windows/Mac users can look <a class="reference external" href="https://www.sylabs.io/guides/3.0/user-guide/installation.html#install-on-windows-or-mac">there</a> and
Ubuntu/Debian users can use directly:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">sudo apt-get install singularity-container</span>
</pre></div>
</div>
</div></blockquote>
</dd>
</dl>
</li>
<li><p>If you <strong>can’t install singularity</strong> on your laptop and you <strong>don’t need apt-get</strong>, you can reserve a <strong>cpu node on the Mila cluster</strong> to build your container.</p></li>
</ul>
<p>In this case, in order to avoid too much I/O over the network, you should define
the singularity cache locally:</p>
<blockquote>
<div><div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">export SINGULARITY_CACHEDIR=$SLURM_TMPDIR</span>
</pre></div>
</div>
</div></blockquote>
<ul class="simple">
<li><dl class="simple">
<dt>If you <strong>can’t install singularity</strong> on your laptop and you <a href="#id5"><span class="problematic" id="id6">**</span></a>want to use</dt><dd><p>apt-get**, you can use <a class="reference external" href="https://www.singularity-hub.org/">singularity-hub</a> to build your containers and read
<a class="reference internal" href="#recipe-section">Recipe_section</a>.</p>
</dd>
</dl>
</li>
</ul>
<section id="download-containers-from-the-web">
<h5>Download containers from the web<a class="headerlink" href="#download-containers-from-the-web" title="Link permanente para este cabeçalho">¶</a></h5>
<p>Hopefully, you may not need to create containers from scratch as many have been
already built for the most common deep learning software. You can find most of
them on <a class="reference external" href="https://hub.docker.com/">dockerhub</a>.</p>
<p>Go on <a class="reference external" href="https://hub.docker.com/">dockerhub</a> and select the container you want to pull.</p>
<p>For example, if you want to get the latest PyTorch version with GPU support
(Replace <em>runtime</em> by <em>devel</em> if you need the full Cuda toolkit):</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity pull docker://pytorch/pytorch:1.0.1-cuda10.0-cudnn7-runtime</span>
</pre></div>
</div>
<p>Or the latest TensorFlow:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity pull docker://tensorflow/tensorflow:latest-gpu-py3</span>
</pre></div>
</div>
<p>Currently the pulled image <code class="docutils literal notranslate"><span class="pre">pytorch.simg</span></code> or <code class="docutils literal notranslate"><span class="pre">tensorflow.simg</span></code> is read-only
meaning that you won’t be able to install anything on it.  Starting now, PyTorch
will be taken as example. If you use TensorFlow, simply replace every
<strong>pytorch</strong> occurrences by <strong>tensorflow</strong>.</p>
</section>
<section id="how-to-add-or-install-stuff-in-a-container">
<h5>How to add or install stuff in a container<a class="headerlink" href="#how-to-add-or-install-stuff-in-a-container" title="Link permanente para este cabeçalho">¶</a></h5>
<p>The first step is to transform your read only container
<code class="docutils literal notranslate"><span class="pre">pytorch-1.0.1-cuda10.0-cudnn7-runtime.simg</span></code> in a writable version that will
allow you to add packages.</p>
<div class="admonition warning">
<p class="admonition-title">Aviso</p>
<p>Depending on the version of singularity you are using, singularity
will build a container with the extension .simg or .sif. If you’re using
.sif files, replace every occurences of .simg by .sif.</p>
</div>
<div class="admonition tip">
<p class="admonition-title">Dica</p>
<p>If you want to use <strong>apt-get</strong> you have to put <strong>sudo</strong> ahead of the
following commands</p>
</div>
<p>This command will create a writable image in the folder <code class="docutils literal notranslate"><span class="pre">pytorch</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity build --sandbox pytorch pytorch-1.0.1-cuda10.0-cudnn7-runtime.simg</span>
</pre></div>
</div>
<p>Then you’ll need the following command to log inside the container.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity shell --writable -H $HOME:/home pytorch</span>
</pre></div>
</div>
<p>Once you get into the container, you can use pip and install anything you need
(Or with <code class="docutils literal notranslate"><span class="pre">apt-get</span></code> if you built the container with sudo).</p>
<div class="admonition warning">
<p class="admonition-title">Aviso</p>
<p>Singularity mounts your home folder, so if you install things into
the <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> of your container, they will be installed in your real
<code class="docutils literal notranslate"><span class="pre">$HOME</span></code>!</p>
</div>
<p>You should install your stuff in /usr/local instead.</p>
</section>
<section id="creating-useful-directories">
<h5>Creating useful directories<a class="headerlink" href="#creating-useful-directories" title="Link permanente para este cabeçalho">¶</a></h5>
<p>One of the benefits of containers is that you’ll be able to use them across
different clusters. However for each cluster the datasets and experiments
folder location can be different. In order to be invariant to those locations,
we will create some useful mount points inside the container:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">mkdir /dataset</span>
<span class="go">mkdir /tmp_log</span>
<span class="go">mkdir /final_log</span>
</pre></div>
</div>
<p>From now, you won’t need to worry anymore when you write your code to specify
where to pick up your dataset. Your dataset will always be in <code class="docutils literal notranslate"><span class="pre">/dataset</span></code>
independently of the cluster you are using.</p>
</section>
<section id="testing">
<h5>Testing<a class="headerlink" href="#testing" title="Link permanente para este cabeçalho">¶</a></h5>
<p>If you have some code that you want to test before finalizing your container,
you have two choices.  You can either log into your container and run Python
code inside it with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity shell --nv pytorch</span>
</pre></div>
</div>
<p>Or you can execute your command directly with</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity exec --nv pytorch Python YOUR_CODE.py</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Dica</p>
<p>—nv allows the container to use gpus. You don’t need this if you
don’t plan to use a gpu.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Aviso</p>
<p>Don’t forget to clear the cache of the packages you installed in
the containers.</p>
</div>
</section>
<section id="creating-a-new-image-from-the-sandbox">
<h5>Creating a new image from the sandbox<a class="headerlink" href="#creating-a-new-image-from-the-sandbox" title="Link permanente para este cabeçalho">¶</a></h5>
<p>Once everything you need is installed inside the container, you need to convert
it back to a read-only singularity image with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity build pytorch_final.simg pytorch</span>
</pre></div>
</div>
</section>
</section>
<section id="second-way-use-recipes">
<span id="recipe-section"></span><h4>Second way: Use recipes<a class="headerlink" href="#second-way-use-recipes" title="Link permanente para este cabeçalho">¶</a></h4>
<p>A singularity recipe is a file including specifics about installation software,
environment variables, files to add, and container metadata.  It is a starting
point for designing any custom container. Instead of pulling a container and
installing your packages manually, you can specify in this file the packages
you want and then build your container from this file.</p>
<p>Here is a toy example of a singularity recipe installing some stuff:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span><span class="c1">################ Header: Define the base system you want to use ################</span>
<span class="gp"># </span>Reference of the kind of base you want to use <span class="o">(</span>e.g., docker, debootstrap, shub<span class="o">)</span>.
<span class="go">Bootstrap: docker</span>
<span class="gp"># </span>Select the docker image you want to use <span class="o">(</span>Here we choose tensorflow<span class="o">)</span>
<span class="go">From: tensorflow/tensorflow:latest-gpu-py3</span>

<span class="gp">#</span><span class="c1">################ Section: Defining the system #################################</span>
<span class="gp"># </span>Commands <span class="k">in</span> the %post section are executed within the container.
<span class="gp">%</span>post
<span class="go">        echo &quot;Installing Tools with apt-get&quot;</span>
<span class="go">        apt-get update</span>
<span class="go">        apt-get install -y cmake libcupti-dev libyaml-dev wget unzip</span>
<span class="go">        apt-get clean</span>
<span class="go">        echo &quot;Installing things with pip&quot;</span>
<span class="go">        pip install tqdm</span>
<span class="go">        echo &quot;Creating mount points&quot;</span>
<span class="go">        mkdir /dataset</span>
<span class="go">        mkdir /tmp_log</span>
<span class="go">        mkdir /final_log</span>


<span class="gp"># </span>Environment variables that should be sourced at runtime.
<span class="gp">%</span>environment
<span class="gp">        # </span>use bash as default shell
<span class="go">        SHELL=/bin/bash</span>
<span class="go">        export SHELL</span>
</pre></div>
</div>
<p>A recipe file contains two parts: the <code class="docutils literal notranslate"><span class="pre">header</span></code> and <code class="docutils literal notranslate"><span class="pre">sections</span></code>. In the
<code class="docutils literal notranslate"><span class="pre">header</span></code> you specify which base system you want to use, it can be any docker
or singularity container. In <code class="docutils literal notranslate"><span class="pre">sections</span></code>, you can list the things you want to
install in the subsection <code class="docutils literal notranslate"><span class="pre">post</span></code> or list the environment’s variable you need
to source at each runtime in the subsection <code class="docutils literal notranslate"><span class="pre">environment</span></code>. For a more detailed
description, please look at the <a class="reference external" href="https://www.sylabs.io/guides/2.6/user-guide/container_recipes.html#container-recipes">singularity documentation</a>.</p>
<p>In order to build a singularity container from a singularity recipe file, you
should use:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">sudo singularity build &lt;NAME_CONTAINER&gt; &lt;YOUR_RECIPE_FILES&gt;</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="admonition-title">Aviso</p>
<p>You always need to use sudo when you build a container from a
recipe. As there is no access to sudo on the cluster, a personal computer or
the use singularity hub is needed to build a container</p>
</div>
<section id="build-recipe-on-singularity-hub">
<h5>Build recipe on singularity hub<a class="headerlink" href="#build-recipe-on-singularity-hub" title="Link permanente para este cabeçalho">¶</a></h5>
<p>Singularity hub allows users to build containers from recipes directly on
singularity-hub’s cloud meaning that you don’t need to build containers by
yourself.  You need to register on <a class="reference external" href="https://www.singularity-hub.org/">singularity-hub</a> and link your
singularity-hub account to your GitHub account, then:</p>
<blockquote>
<div><blockquote>
<div><ol class="arabic simple">
<li><p>Create a new github repository.</p></li>
<li><p>Add a collection on <a class="reference external" href="https://www.singularity-hub.org/">singularity-hub</a> and select the github repository your created.</p></li>
<li><p>Clone the github repository on your computer.</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ git clone &lt;url&gt;
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple">
<li><p>Write the singularity recipe and save it as a file named <strong>Singularity</strong>.</p></li>
<li><p>Git add <strong>Singularity</strong>, commit and push on the master branch</p></li>
</ol>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ git add Singularity
$ git commit
$ git push origin master
</pre></div>
</div>
</div></blockquote>
</div></blockquote>
<p>At this point, robots from singularity-hub will build the container for you, you
will be able to download your container from the website or directly with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity pull shub://&lt;github_username&gt;/&lt;repository_name&gt;</span>
</pre></div>
</div>
</section>
<section id="example-recipe-with-openai-gym-mujoco-and-miniworld">
<h5>Example: Recipe with OpenAI gym, MuJoCo and Miniworld<a class="headerlink" href="#example-recipe-with-openai-gym-mujoco-and-miniworld" title="Link permanente para este cabeçalho">¶</a></h5>
<p>Here is an example on how you can use a singularity recipe to install complex
environment such as OpenAI gym, MuJoCo and Miniworld on a PyTorch based
container. In order to use MuJoCo, you’ll need to copy the key stored on the
Mila cluster in <cite>/ai/apps/mujoco/license/mjkey.txt</cite> to your current directory.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>This is a dockerfile that sets up a full Gym install with <span class="nb">test</span> dependencies
<span class="go">Bootstrap: docker</span>

<span class="gp"># </span>Here we ll build our container upon the pytorch container
<span class="go">From: pytorch/pytorch:1.0-cuda10.0-cudnn7-runtime</span>

<span class="gp"># </span>Now we<span class="s1">&#39;ll copy the mjkey file located in the current directory inside the container&#39;</span>s root
<span class="gp"># </span>directory
<span class="gp">%</span>files
<span class="go">        mjkey.txt</span>

<span class="gp"># </span>Then we put everything we need to install
<span class="gp">%</span>post
<span class="go">        export PATH=$PATH:/opt/conda/bin</span>
<span class="go">        apt -y update &amp;&amp; \</span>
<span class="go">        apt install -y keyboard-configuration &amp;&amp; \</span>
<span class="go">        apt install -y \</span>
<span class="go">        python3-dev \</span>
<span class="go">        python-pyglet \</span>
<span class="go">        python3-opengl \</span>
<span class="go">        libhdf5-dev \</span>
<span class="go">        libjpeg-dev \</span>
<span class="go">        libboost-all-dev \</span>
<span class="go">        libsdl2-dev \</span>
<span class="go">        libosmesa6-dev \</span>
<span class="go">        patchelf \</span>
<span class="go">        ffmpeg \</span>
<span class="go">        xvfb \</span>
<span class="go">        libhdf5-dev \</span>
<span class="go">        openjdk-8-jdk \</span>
<span class="go">        wget \</span>
<span class="go">        git \</span>
<span class="go">        unzip &amp;&amp; \</span>
<span class="go">        apt clean &amp;&amp; \</span>
<span class="go">        rm -rf /var/lib/apt/lists/*</span>
<span class="go">        pip install h5py</span>

<span class="gp">        # </span>Download Gym and MuJoCo
<span class="go">        mkdir /Gym &amp;&amp; cd /Gym</span>
<span class="go">        git clone https://github.com/openai/gym.git || true &amp;&amp; \</span>
<span class="go">        mkdir /Gym/.mujoco &amp;&amp; cd /Gym/.mujoco</span>
<span class="go">        wget https://www.roboti.us/download/mjpro150_linux.zip  &amp;&amp; \</span>
<span class="go">        unzip mjpro150_linux.zip &amp;&amp; \</span>
<span class="go">        wget https://www.roboti.us/download/mujoco200_linux.zip &amp;&amp; \</span>
<span class="go">        unzip mujoco200_linux.zip &amp;&amp; \</span>
<span class="go">        mv mujoco200_linux mujoco200</span>

<span class="gp">        # </span>Export global environment variables
<span class="go">        export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt</span>
<span class="go">        export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/</span>
<span class="go">        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin</span>
<span class="go">        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin</span>
<span class="go">        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin</span>
<span class="go">        cp /mjkey.txt /Gym/.mujoco/mjkey.txt</span>
<span class="gp">        # </span>Install Python dependencies
<span class="go">        wget https://raw.githubusercontent.com/openai/mujoco-py/master/requirements.txt</span>
<span class="go">        pip install -r requirements.txt</span>
<span class="gp">        # </span>Install Gym and MuJoCo
<span class="go">        cd /Gym/gym</span>
<span class="go">        pip install -e &#39;.[all]&#39;</span>
<span class="gp">        # </span>Change permission to use mujoco_py as non sudoer user
<span class="go">        chmod -R 777 /opt/conda/lib/python3.6/site-packages/mujoco_py/</span>
<span class="go">        pip install --upgrade minerl</span>

<span class="gp"># </span>Export global environment variables
<span class="gp">%</span>environment
<span class="go">        export SHELL=/bin/sh</span>
<span class="go">        export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt</span>
<span class="go">        export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/</span>
<span class="go">        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin</span>
<span class="go">        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin</span>
<span class="go">        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin</span>
<span class="go">        export PATH=/Gym/gym/.tox/py3/bin:$PATH</span>

<span class="gp">%</span>runscript
<span class="go">        exec /bin/sh &quot;$@&quot;</span>
</pre></div>
</div>
<p>Here is the same recipe but written for TensorFlow:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>This is a dockerfile that sets up a full Gym install with <span class="nb">test</span> dependencies
<span class="go">Bootstrap: docker</span>

<span class="gp"># </span>Here we ll build our container upon the tensorflow container
<span class="go">From: tensorflow/tensorflow:latest-gpu-py3</span>

<span class="gp"># </span>Now we<span class="s1">&#39;ll copy the mjkey file located in the current directory inside the container&#39;</span>s root
<span class="gp"># </span>directory
<span class="gp">%</span>files
<span class="go">        mjkey.txt</span>

<span class="gp"># </span>Then we put everything we need to install
<span class="gp">%</span>post
<span class="go">        apt -y update &amp;&amp; \</span>
<span class="go">        apt install -y keyboard-configuration &amp;&amp; \</span>
<span class="go">        apt install -y \</span>
<span class="go">        python3-setuptools \</span>
<span class="go">        python3-dev \</span>
<span class="go">        python-pyglet \</span>
<span class="go">        python3-opengl \</span>
<span class="go">        libjpeg-dev \</span>
<span class="go">        libboost-all-dev \</span>
<span class="go">        libsdl2-dev \</span>
<span class="go">        libosmesa6-dev \</span>
<span class="go">        patchelf \</span>
<span class="go">        ffmpeg \</span>
<span class="go">        xvfb \</span>
<span class="go">        wget \</span>
<span class="go">        git \</span>
<span class="go">        unzip &amp;&amp; \</span>
<span class="go">        apt clean &amp;&amp; \</span>
<span class="go">        rm -rf /var/lib/apt/lists/*</span>

<span class="gp">        # </span>Download Gym and MuJoCo
<span class="go">        mkdir /Gym &amp;&amp; cd /Gym</span>
<span class="go">        git clone https://github.com/openai/gym.git || true &amp;&amp; \</span>
<span class="go">        mkdir /Gym/.mujoco &amp;&amp; cd /Gym/.mujoco</span>
<span class="go">        wget https://www.roboti.us/download/mjpro150_linux.zip  &amp;&amp; \</span>
<span class="go">        unzip mjpro150_linux.zip &amp;&amp; \</span>
<span class="go">        wget https://www.roboti.us/download/mujoco200_linux.zip &amp;&amp; \</span>
<span class="go">        unzip mujoco200_linux.zip &amp;&amp; \</span>
<span class="go">        mv mujoco200_linux mujoco200</span>

<span class="gp">        # </span>Export global environment variables
<span class="go">        export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt</span>
<span class="go">        export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/</span>
<span class="go">        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin</span>
<span class="go">        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin</span>
<span class="go">        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin</span>
<span class="go">        cp /mjkey.txt /Gym/.mujoco/mjkey.txt</span>

<span class="gp">        # </span>Install Python dependencies
<span class="go">        wget https://raw.githubusercontent.com/openai/mujoco-py/master/requirements.txt</span>
<span class="go">        pip install -r requirements.txt</span>
<span class="gp">        # </span>Install Gym and MuJoCo
<span class="go">        cd /Gym/gym</span>
<span class="go">        pip install -e &#39;.[all]&#39;</span>
<span class="gp">        # </span>Change permission to use mujoco_py as non sudoer user
<span class="go">        chmod -R 777 /usr/local/lib/python3.5/dist-packages/mujoco_py/</span>

<span class="gp">        # </span>Then install miniworld
<span class="go">        cd /usr/local/</span>
<span class="go">        git clone https://github.com/maximecb/gym-miniworld.git</span>
<span class="go">        cd gym-miniworld</span>
<span class="go">        pip install -e .</span>

<span class="gp"># </span>Export global environment variables
<span class="gp">%</span>environment
<span class="go">        export SHELL=/bin/bash</span>
<span class="go">        export MUJOCO_PY_MJKEY_PATH=/Gym/.mujoco/mjkey.txt</span>
<span class="go">        export MUJOCO_PY_MUJOCO_PATH=/Gym/.mujoco/mujoco150/</span>
<span class="go">        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mjpro150/bin</span>
<span class="go">        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/Gym/.mujoco/mujoco200/bin</span>
<span class="go">        export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/bin</span>
<span class="go">        export PATH=/Gym/gym/.tox/py3/bin:$PATH</span>

<span class="gp">%</span>runscript
<span class="go">        exec /bin/bash &quot;$@&quot;</span>
</pre></div>
</div>
<p>Keep in mind that those environment variables are sourced at runtime and not at
build time. This is why, you should also define them in the <code class="docutils literal notranslate"><span class="pre">%post</span></code> section
since they are required to install MuJoCo.</p>
</section>
</section>
</section>
<section id="using-containers-on-clusters">
<span id="using-containers"></span><h3>Using containers on clusters<a class="headerlink" href="#using-containers-on-clusters" title="Link permanente para este cabeçalho">¶</a></h3>
<section id="how-to-use-containers-on-clusters">
<h4>How to use containers on clusters<a class="headerlink" href="#how-to-use-containers-on-clusters" title="Link permanente para este cabeçalho">¶</a></h4>
<p>On every cluster with Slurm, datasets and intermediate results should go in
<code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> while the final experiment results should go in <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code>.
In order to use the container you built, you need to copy it on the cluster you
want to use.</p>
<div class="admonition warning">
<p class="admonition-title">Aviso</p>
<p>You should always store your container in $SCRATCH !</p>
</div>
<p>Then reserve a node with srun/sbatch, copy the container and your dataset on the
node given by SLURM (i.e in <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code>) and execute the code
<code class="docutils literal notranslate"><span class="pre">&lt;YOUR_CODE&gt;</span></code> within the container <code class="docutils literal notranslate"><span class="pre">&lt;YOUR_CONTAINER&gt;</span></code> with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity exec --nv -H $HOME:/home -B $SLURM_TMPDIR:/dataset/ -B $SLURM_TMPDIR:/tmp_log/ -B $SCRATCH:/final_log/ $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; python &lt;YOUR_CODE&gt;</span>
</pre></div>
</div>
<p>Remember that <code class="docutils literal notranslate"><span class="pre">/dataset</span></code>, <code class="docutils literal notranslate"><span class="pre">/tmp_log</span></code> and <code class="docutils literal notranslate"><span class="pre">/final_log</span></code> were created in the
previous section. Now each time, we’ll use singularity, we are explicitly
telling it to mount <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> on the cluster’s node in the folder
<code class="docutils literal notranslate"><span class="pre">/dataset</span></code> inside the container with the option <code class="docutils literal notranslate"><span class="pre">-B</span></code> such that each dataset
downloaded by PyTorch in <code class="docutils literal notranslate"><span class="pre">/dataset</span></code> will be available in <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code>.</p>
<p>This will allow us to have code and scripts that are invariant to the cluster
environment. The option <code class="docutils literal notranslate"><span class="pre">-H</span></code> specify what will be the container’s home. For
example, if you have your code in <code class="docutils literal notranslate"><span class="pre">$HOME/Project12345/Version35/</span></code> you can
specify <code class="docutils literal notranslate"><span class="pre">-H</span> <span class="pre">$HOME/Project12345/Version35:/home</span></code>, thus the container will only
have access to the code inside <code class="docutils literal notranslate"><span class="pre">Version35</span></code>.</p>
<p>If you want to run multiple commands inside the container you can use:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity exec --nv -H $HOME:/home -B $SLURM_TMPDIR:/dataset/ \</span>
<span class="go">    -B $SLURM_TMPDIR:/tmp_log/ -B $SCRATCH:/final_log/ \</span>
<span class="gp">    $</span>SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; bash -c <span class="s1">&#39;pwd &amp;&amp; ls &amp;&amp; python &lt;YOUR_CODE&gt;&#39;</span>
</pre></div>
</div>
<section id="example-interactive-case-srun-salloc">
<h5>Example: Interactive case (srun/salloc)<a class="headerlink" href="#example-interactive-case-srun-salloc" title="Link permanente para este cabeçalho">¶</a></h5>
<p>Once you get an interactive session with SLURM, copy <code class="docutils literal notranslate"><span class="pre">&lt;YOUR_CONTAINER&gt;</span></code> and
<code class="docutils literal notranslate"><span class="pre">&lt;YOUR_DATASET&gt;</span></code> to <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp"># </span><span class="m">0</span>. Get an interactive session
<span class="gp">$ </span>srun --gres<span class="o">=</span>gpu:1
<span class="gp"># </span><span class="m">1</span>. Copy your container on the compute node
<span class="gp">$ </span>rsync -avz <span class="nv">$SCRATCH</span>/&lt;YOUR_CONTAINER&gt; <span class="nv">$SLURM_TMPDIR</span>
<span class="gp"># </span><span class="m">2</span>. Copy your dataset on the compute node
<span class="gp">$ </span>rsync -avz <span class="nv">$SCRATCH</span>/&lt;YOUR_DATASET&gt; <span class="nv">$SLURM_TMPDIR</span>
</pre></div>
</div>
<p>Then use <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">shell</span></code> to get a shell inside the container</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp"># </span><span class="m">3</span>. Get a shell <span class="k">in</span> your environment
<span class="gp">$ </span>singularity shell --nv <span class="se">\</span>
        -H <span class="nv">$HOME</span>:/home <span class="se">\</span>
        -B <span class="nv">$SLURM_TMPDIR</span>:/dataset/ <span class="se">\</span>
        -B <span class="nv">$SLURM_TMPDIR</span>:/tmp_log/ <span class="se">\</span>
        -B <span class="nv">$SCRATCH</span>:/final_log/ <span class="se">\</span>
<span class="gp">        $</span>SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt;
</pre></div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp"># </span><span class="m">4</span>. Execute your code
<span class="go">&lt;Singularity_container&gt;$ python &lt;YOUR_CODE&gt;</span>
</pre></div>
</div>
<p><strong>or</strong> use <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">exec</span></code> to execute <code class="docutils literal notranslate"><span class="pre">&lt;YOUR_CODE&gt;</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp"># </span><span class="m">3</span>. Execute your code
<span class="gp">$ </span>singularity <span class="nb">exec</span> --nv <span class="se">\</span>
        -H <span class="nv">$HOME</span>:/home <span class="se">\</span>
        -B <span class="nv">$SLURM_TMPDIR</span>:/dataset/ <span class="se">\</span>
        -B <span class="nv">$SLURM_TMPDIR</span>:/tmp_log/ <span class="se">\</span>
        -B <span class="nv">$SCRATCH</span>:/final_log/ <span class="se">\</span>
<span class="gp">        $</span>SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; <span class="se">\</span>
        python &lt;YOUR_CODE&gt;
</pre></div>
</div>
<p>You can create also the following alias to make your life easier.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span>alias my_env=&#39;singularity exec --nv \
        -H $HOME:/home \
        -B $SLURM_TMPDIR:/dataset/ \
        -B $SLURM_TMPDIR:/tmp_log/ \
        -B $SCRATCH:/final_log/ \
        $SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt;&#39;
</pre></div>
</div>
<p>This will allow you to run any code with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">my_env python &lt;YOUR_CODE&gt;</span>
</pre></div>
</div>
</section>
<section id="example-sbatch-case">
<h5>Example: sbatch case<a class="headerlink" href="#example-sbatch-case" title="Link permanente para este cabeçalho">¶</a></h5>
<p>You can also create a <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> script:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>!/bin/bash
<span class="gp">#</span>SBATCH --cpus-per-task<span class="o">=</span><span class="m">6</span>         <span class="c1"># Ask for 6 CPUs</span>
<span class="gp">#</span>SBATCH --gres<span class="o">=</span>gpu:1              <span class="c1"># Ask for 1 GPU</span>
<span class="gp">#</span>SBATCH --mem<span class="o">=</span>10G                 <span class="c1"># Ask for 10 GB of RAM</span>
<span class="gp">#</span>SBATCH --time<span class="o">=</span><span class="m">0</span>:10:00            <span class="c1"># The job will run for 10 minutes</span>

<span class="gp"># </span><span class="m">1</span>. Copy your container on the compute node
<span class="go">rsync -avz $SCRATCH/&lt;YOUR_CONTAINER&gt; $SLURM_TMPDIR</span>
<span class="gp"># </span><span class="m">2</span>. Copy your dataset on the compute node
<span class="go">rsync -avz $SCRATCH/&lt;YOUR_DATASET&gt; $SLURM_TMPDIR</span>
<span class="gp"># </span><span class="m">3</span>. Executing your code with singularity
<span class="go">singularity exec --nv \</span>
<span class="go">        -H $HOME:/home \</span>
<span class="go">        -B $SLURM_TMPDIR:/dataset/ \</span>
<span class="go">        -B $SLURM_TMPDIR:/tmp_log/ \</span>
<span class="go">        -B $SCRATCH:/final_log/ \</span>
<span class="gp">        $</span>SLURM_TMPDIR/&lt;YOUR_CONTAINER&gt; <span class="se">\</span>
        python <span class="s2">&quot;&lt;YOUR_CODE&gt;&quot;</span>
<span class="gp"># </span><span class="m">4</span>. Copy whatever you want to save on <span class="nv">$SCRATCH</span>
<span class="go">rsync -avz $SLURM_TMPDIR/&lt;to_save&gt; $SCRATCH</span>
</pre></div>
</div>
</section>
<section id="issue-with-pybullet-and-opengl-libraries">
<h5>Issue with PyBullet and OpenGL libraries<a class="headerlink" href="#issue-with-pybullet-and-opengl-libraries" title="Link permanente para este cabeçalho">¶</a></h5>
<p>If you are running certain gym environments that require <code class="docutils literal notranslate"><span class="pre">pyglet</span></code>, you may
encounter a problem when running your singularity instance with the Nvidia
drivers using the <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flag. This happens because the <code class="docutils literal notranslate"><span class="pre">--nv</span></code> flag also
provides the OpenGL libraries:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">libGL.so.1 =&gt; /.singularity.d/libs/libGL.so.1</span>
<span class="go">libGLX.so.0 =&gt; /.singularity.d/libs/libGLX.so.0</span>
</pre></div>
</div>
<p>If you don’t experience those problems with <code class="docutils literal notranslate"><span class="pre">pyglet</span></code>, you probably don’t need
to address this. Otherwise, you can resolve those problems by <code class="docutils literal notranslate"><span class="pre">apt-get</span> <span class="pre">install</span>
<span class="pre">-y</span> <span class="pre">libosmesa6-dev</span> <span class="pre">mesa-utils</span> <span class="pre">mesa-utils-extra</span> <span class="pre">libgl1-mesa-glx</span></code>, and then making
sure that your <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> points to those libraries before the ones in
<code class="docutils literal notranslate"><span class="pre">/.singularity.d/libs</span></code>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">%</span>environment
<span class="gp">        # </span>...
<span class="go">        export LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/mesa:$LD_LIBRARY_PATH</span>
</pre></div>
</div>
</section>
<section id="apuana-cluster">
<h5>Apuana cluster<a class="headerlink" href="#apuana-cluster" title="Link permanente para este cabeçalho">¶</a></h5>
<p>On the Apuana cluster <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> is not yet defined, you should add the
experiment results you want to keep in <code class="docutils literal notranslate"><span class="pre">/network/scratch/&lt;u&gt;/&lt;username&gt;/</span></code>. In
order to use the sbatch script above and to match other cluster environment’s
names, you can define <code class="docutils literal notranslate"><span class="pre">$SCRATCH</span></code> as an alias for
<code class="docutils literal notranslate"><span class="pre">/network/scratch/&lt;u&gt;/&lt;username&gt;</span></code> with:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">echo &quot;export SCRATCH=/network/scratch/${USER:0:1}/$USER&quot; &gt;&gt; ~/.bashrc</span>
</pre></div>
</div>
<p>Then, you can follow the general procedure explained above.</p>
</section>
</section>
</section>
</section>
<section id="sharing-data-with-acls">
<h2>Sharing Data with ACLs<a class="headerlink" href="#sharing-data-with-acls" title="Link permanente para este cabeçalho">¶</a></h2>
<p>Regular permissions bits are extremely blunt tools: They control access through
only three sets of bits owning user, owning group and all others. Therefore,
access is either too narrow (<code class="docutils literal notranslate"><span class="pre">0700</span></code> allows access only by oneself) or too wide
(<code class="docutils literal notranslate"><span class="pre">770</span></code> gives all permissions to everyone in the same group, and <code class="docutils literal notranslate"><span class="pre">777</span></code> to
literally everyone).</p>
<p>ACLs (Access Control Lists) are an expansion of the permissions bits that allow
more fine-grained, granular control of accesses to a file. They can be used to
permit specific users access to files and folders even if conservative default
permissions would have denied them such access.</p>
<p>As an illustrative example, to use ACLs to allow <code class="docutils literal notranslate"><span class="pre">$USER</span></code> (<strong>oneself</strong>) to
share with <code class="docutils literal notranslate"><span class="pre">$USER2</span></code> (<strong>another person</strong>) a “playground” folder hierarchy in
Mila’s scratch filesystem at a location</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">/network/scratch/${USER:0:1}/$USER/X/Y/Z/...</span></code></p>
</div></blockquote>
<p>in a safe and secure fashion that allows both users to read, write, execute,
search and delete each others’ files:</p>
<hr class="docutils" />
<div class="line-block">
<div class="line"><strong>1.</strong> Grant <strong>oneself</strong> permissions to access any <strong>future</strong> files/folders created
by the other <em>(or oneself)</em></div>
<div class="line">(<code class="docutils literal notranslate"><span class="pre">-d</span></code> renders this permission a “default” / inheritable one)</div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">setfacl -Rdm user:${USER}:rwx  /network/scratch/${USER:0:1}/$USER/X/Y/Z/</span>
</pre></div>
</div>
<hr class="docutils" />
<div class="admonition note">
<p class="admonition-title">Nota</p>
<blockquote>
<div><p>The importance of doing this seemingly-redundant step first is that files</p>
</div></blockquote>
<dl class="simple">
<dt>and folders are <strong>always</strong> owned by only one person, almost always their</dt><dd><p>creator (the UID will be the creator’s, the GID typically as well). If that
user is not yourself, you will not have access to those files unless the
other person specifically gives them to you – or these files inherited a
default ACL allowing you full access.</p>
</dd>
</dl>
<p><strong>This</strong> is the inherited, default ACL serving that purpose.</p>
</div>
<div class="line-block">
<div class="line"><strong>2.</strong> Grant <strong>the other</strong> permission to access any <strong>future</strong> files/folders created
by the other <em>(or oneself)</em></div>
<div class="line">(<code class="docutils literal notranslate"><span class="pre">-d</span></code> renders this permission a “default” / inheritable one)</div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">setfacl -Rdm user:${USER2}:rwx /network/scratch/${USER:0:1}/$USER/X/Y/Z/</span>
</pre></div>
</div>
<hr class="docutils" />
<div class="line-block">
<div class="line"><strong>3.</strong> Grant <strong>the other</strong> permission to access any <strong>existing</strong> files/folders created
by <em>oneself</em>.</div>
<div class="line">Such files and folders were created before the new default ACLs were added
above and thus did not inherit them from their parent folder at the moment of
their creation.</div>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">setfacl -Rm  user:${USER2}:rwx /network/scratch/${USER:0:1}/$USER/X/Y/Z/</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>The purpose of granting permissions first for <em>future</em> files and then for
<em>existing</em> files is to prevent a <strong>race condition</strong> whereby after the first <code class="docutils literal notranslate"><span class="pre">setfacl</span></code> command the other person could create files to which the
second <code class="docutils literal notranslate"><span class="pre">setfacl</span></code> command does not apply.</p>
</div>
<hr class="docutils" />
<div class="line-block">
<div class="line"><strong>4.</strong> Grant <strong>another</strong> permission to search through one’s hierarchy down to the
shared location in question.</div>
</div>
<ul class="simple">
<li><p><strong>Non</strong>-recursive (!!!!)</p></li>
<li><dl class="simple">
<dt>May also grant <code class="docutils literal notranslate"><span class="pre">:rx</span></code> in unlikely event others listing your folders on the</dt><dd><p>path is not troublesome or desirable.</p>
</dd>
</dl>
</li>
</ul>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">setfacl -m   user:${USER2}:x   /network/scratch/${USER:0:1}/$USER/X/Y/</span>
<span class="go">setfacl -m   user:${USER2}:x   /network/scratch/${USER:0:1}/$USER/X/</span>
<span class="go">setfacl -m   user:${USER2}:x   /network/scratch/${USER:0:1}/$USER/</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<blockquote>
<div><p>In order to access a file, all folders from the root (<code class="docutils literal notranslate"><span class="pre">/</span></code>) down to the
parent folder in question must be searchable (<code class="docutils literal notranslate"><span class="pre">+x</span></code>) by the concerned user.
This is already the case for all users for folders such as <code class="docutils literal notranslate"><span class="pre">/</span></code>,
<code class="docutils literal notranslate"><span class="pre">/network</span></code> and <code class="docutils literal notranslate"><span class="pre">/network/scratch</span></code>, but users must explicitly grant access
to some or all users either through base permissions or by adding ACLs, for
at least <code class="docutils literal notranslate"><span class="pre">/network/scratch/${USER:0:1}/$USER</span></code>, <code class="docutils literal notranslate"><span class="pre">$HOME</span></code> and subfolders.</p>
</div></blockquote>
<dl>
<dt>To bluntly allow <strong>all</strong> users to search through a folder (<strong>think twice!</strong>),</dt><dd><p>the following command can be used:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">chmod a+x /network/scratch/${USER:0:1}/$USER/</span>
</pre></div>
</div>
</dd>
</dl>
</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<blockquote>
<div><p>For more information on <code class="docutils literal notranslate"><span class="pre">setfacl</span></code> and path resolution/access checking,
consider the following documentation viewing commands:</p>
</div></blockquote>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">setfacl</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">path_resolution</span></code></p></li>
</ul>
</div>
<section id="viewing-and-verifying-acls">
<h3>Viewing and Verifying ACLs<a class="headerlink" href="#viewing-and-verifying-acls" title="Link permanente para este cabeçalho">¶</a></h3>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">getfacl /path/to/folder/or/file</span>
<span class="go">            1:  # file: somedir/</span>
<span class="go">            2:  # owner: lisa</span>
<span class="go">            3:  # group: staff</span>
<span class="go">            4:  # flags: -s-</span>
<span class="go">            5:  user::rwx</span>
<span class="go">            6:  user:joe:rwx               #effective:r-x</span>
<span class="go">            7:  group::rwx                 #effective:r-x</span>
<span class="go">            8:  group:cool:r-x</span>
<span class="go">            9:  mask::r-x</span>
<span class="go">            10:  other::r-x</span>
<span class="go">            11:  default:user::rwx</span>
<span class="go">            12:  default:user:joe:rwx       #effective:r-x</span>
<span class="go">            13:  default:group::r-x</span>
<span class="go">            14:  default:mask::r-x</span>
<span class="go">            15:  default:other::---</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">getfacl</span></code></p></li>
</ul>
</div>
</section>
</section>
<section id="advanced-slurm-usage-and-multiple-gpu-jobs">
<h2>Advanced SLURM usage and Multiple GPU jobs<a class="headerlink" href="#advanced-slurm-usage-and-multiple-gpu-jobs" title="Link permanente para este cabeçalho">¶</a></h2>
<section id="handling-preemption">
<h3>Handling preemption<a class="headerlink" href="#handling-preemption" title="Link permanente para este cabeçalho">¶</a></h3>
<p id="advanced-preemption">On the Apuana cluster, jobs can preempt one-another depending on their priority
(unkillable&gt;high&gt;low) (See the <a class="reference external" href="https://slurm.schedmd.com/preempt.html">Slurm documentation</a>)</p>
<p>The default preemption mechanism is to kill and re-queue the job automatically
without any notice. To allow a different preemption mechanism, every partition
have been duplicated (i.e. have the same characteristics as their counterparts)
allowing a <strong>120sec</strong> grace period before killing your job <em>but don’t requeue
it automatically</em>: those partitions are referred by the suffix: <code class="docutils literal notranslate"><span class="pre">-grace</span></code>
(<code class="docutils literal notranslate"><span class="pre">main-grace,</span> <span class="pre">long-grace,</span> <span class="pre">main-cpu-grace,</span> <span class="pre">long-cpu-grace</span></code>).</p>
<p>When using a partition with a grace period, a series of signals consisting of
first <code class="docutils literal notranslate"><span class="pre">SIGCONT</span></code> and <code class="docutils literal notranslate"><span class="pre">SIGTERM</span></code> then <code class="docutils literal notranslate"><span class="pre">SIGKILL</span></code> will be sent to the SLURM
job.  It’s good practice to catch those signals using the Linux <code class="docutils literal notranslate"><span class="pre">trap</span></code> command
to properly terminate a job and save what’s necessary to restart the job.  On
each cluster, you’ll be allowed a <em>grace period</em> before SLURM actually kills
your job (<code class="docutils literal notranslate"><span class="pre">SIGKILL</span></code>).</p>
<p>The easiest way to handle preemption is by trapping the <code class="docutils literal notranslate"><span class="pre">SIGTERM</span></code> signal</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>SBATCH --ntasks<span class="o">=</span><span class="m">1</span>
<span class="gp">#</span>SBATCH ....

<span class="go">exit_script() {</span>
<span class="go">    echo &quot;Preemption signal, saving myself&quot;</span>
<span class="go">    trap - SIGTERM # clear the trap</span>
<span class="gp">    # </span>Optional: sends SIGTERM to child/sub processes
<span class="go">    kill -- -$$</span>
<span class="go">}</span>

<span class="go">trap exit_script SIGTERM</span>

<span class="gp"># </span>The main script part
<span class="go">python3 my_script</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<div class="line-block">
<div class="line"><strong>Requeuing</strong>:</div>
<div class="line">The Slurm scheduler on the cluster does not allow a grace period before</div>
<div class="line">preempting a job while requeuing it automatically, therefore your job will</div>
<div class="line">be cancelled at the end of the grace period.</div>
<div class="line">To automatically requeue it, you can just add the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> command inside</div>
<div class="line">your <code class="docutils literal notranslate"><span class="pre">exit_script</span></code> function.</div>
</div>
</div>
</section>
<section id="packing-jobs">
<h3>Packing jobs<a class="headerlink" href="#packing-jobs" title="Link permanente para este cabeçalho">¶</a></h3>
<section id="sharing-a-gpu-between-processes">
<h4>Sharing a GPU between processes<a class="headerlink" href="#sharing-a-gpu-between-processes" title="Link permanente para este cabeçalho">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">srun</span></code>, when used in a batch job is responsible for starting tasks on the
allocated resources (see srun) SLURM batch script</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>SBATCH --ntasks-per-node<span class="o">=</span><span class="m">2</span>
<span class="gp">#</span>SBATCH --output<span class="o">=</span>myjob_output_wrapper.out
<span class="gp">#</span>SBATCH --ntasks<span class="o">=</span><span class="m">2</span>
<span class="gp">#</span>SBATCH --gres<span class="o">=</span>gpu:1
<span class="gp">#</span>SBATCH --cpus-per-task<span class="o">=</span><span class="m">4</span>
<span class="gp">#</span>SBATCH --mem<span class="o">=</span>18G
<span class="go">srun -l --output=myjob_output_%t.out python script args</span>
</pre></div>
</div>
<p>This will run Python 2 times, each process with 4 CPUs with the same arguments
<code class="docutils literal notranslate"><span class="pre">--output=myjob_output_%t.out</span></code> will create 2 output files appending the task
id (<code class="docutils literal notranslate"><span class="pre">%t</span></code>) to the filename and 1 global log file for things happening outside
the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command.</p>
<p>Knowing that, if you want to have 2 different arguments to the Python program,
you can use a multi-prog configuration file: <code class="docutils literal notranslate"><span class="pre">srun</span> <span class="pre">-l</span> <span class="pre">--multi-prog</span> <span class="pre">silly.conf</span></code></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">0  python script firstarg</span>
<span class="go">1  python script secondarg</span>
</pre></div>
</div>
<p>Or by specifying a range of tasks</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">0-1  python script %t</span>
</pre></div>
</div>
<p>%t being the taskid that your Python script will parse.  Note the <code class="docutils literal notranslate"><span class="pre">-l</span></code> on the
<code class="docutils literal notranslate"><span class="pre">srun</span></code> command: this will prepend each line with the taskid (0:, 1:)</p>
</section>
<section id="sharing-a-node-with-multiple-gpu-1process-gpu">
<h4>Sharing a node with multiple GPU 1process/GPU<a class="headerlink" href="#sharing-a-node-with-multiple-gpu-1process-gpu" title="Link permanente para este cabeçalho">¶</a></h4>
<p>On Digital Research Alliance of Canada, several nodes, especially nodes with
<code class="docutils literal notranslate"><span class="pre">largeGPU</span></code> (P100) are reserved for jobs requesting the whole node, therefore
packing multiple processes in a single job can leverage faster GPU.</p>
<p>If you want different tasks to access different GPUs in a single allocation you
need to create an allocation requesting a whole node and using <code class="docutils literal notranslate"><span class="pre">srun</span></code> with a
subset of those resources (1 GPU).</p>
<p>Keep in mind that every resource not specified on the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command while
inherit the global allocation specification so you need to split each resource
in a subset (except –cpu-per-task which is a per-task requirement)</p>
<p>Each <code class="docutils literal notranslate"><span class="pre">srun</span></code> represents a job step (<code class="docutils literal notranslate"><span class="pre">%s</span></code>).</p>
<p>Example for a GPU node with 24 cores and 4 GPUs and 128G of RAM
Requesting 1 task per GPU</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>!/bin/bash
<span class="gp">#</span>SBATCH --nodes<span class="o">=</span><span class="m">1</span>-1
<span class="gp">#</span>SBATCH --ntasks-per-node<span class="o">=</span><span class="m">4</span>
<span class="gp">#</span>SBATCH --output<span class="o">=</span>myjob_output_wrapper.out
<span class="gp">#</span>SBATCH --gres<span class="o">=</span>gpu:4
<span class="gp">#</span>SBATCH --cpus-per-task<span class="o">=</span><span class="m">6</span>
<span class="go">srun --gres=gpu:1 -n1 --mem=30G -l --output=%j-step-%s.out --exclusive --multi-prog python script args1 &amp;</span>
<span class="go">srun --gres=gpu:1 -n1 --mem=30G -l --output=%j-step-%s.out --exclusive --multi-prog python script args2 &amp;</span>
<span class="go">srun --gres=gpu:1 -n1 --mem=30G -l --output=%j-step-%s.out --exclusive --multi-prog python script args3 &amp;</span>
<span class="go">srun --gres=gpu:1 -n1 --mem=30G -l --output=%j-step-%s.out --exclusive --multi-prog python script args4 &amp;</span>
<span class="go">wait</span>
</pre></div>
</div>
<p>This will create 4 output files:</p>
<ul class="simple">
<li><p>JOBID-step-0.out</p></li>
<li><p>JOBID-step-1.out</p></li>
<li><p>JOBID-step-2.out</p></li>
<li><p>JOBID-step-3.out</p></li>
</ul>
</section>
<section id="sharing-a-node-with-multiple-gpu-multiple-processes-gpu">
<h4>Sharing a node with multiple GPU &amp; multiple processes/GPU<a class="headerlink" href="#sharing-a-node-with-multiple-gpu-multiple-processes-gpu" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Combining both previous sections, we can create a script requesting a whole node
with four GPUs, allocating 1 GPU per <code class="docutils literal notranslate"><span class="pre">srun</span></code> and sharing each GPU with multiple
processes</p>
<p>Example still with a 24 cores/4 GPUs/128G RAM
Requesting 2 tasks per GPU</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>!/bin/bash
<span class="gp">#</span>SBATCH --nodes<span class="o">=</span><span class="m">1</span>-1
<span class="gp">#</span>SBATCH --ntasks-per-node<span class="o">=</span><span class="m">8</span>
<span class="gp">#</span>SBATCH --output<span class="o">=</span>myjob_output_wrapper.out
<span class="gp">#</span>SBATCH --gres<span class="o">=</span>gpu:4
<span class="gp">#</span>SBATCH --cpus-per-task<span class="o">=</span><span class="m">3</span>
<span class="go">srun --gres=gpu:1 -n2 --mem=30G -l --output=%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf &amp;</span>
<span class="go">srun --gres=gpu:1 -n2 --mem=30G -l --output=%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf &amp;</span>
<span class="go">srun --gres=gpu:1 -n2 --mem=30G -l --output=%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf &amp;</span>
<span class="go">srun --gres=gpu:1 -n2 --mem=30G -l --output=%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf &amp;</span>
<span class="go">wait</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--exclusive</span></code> is important to specify subsequent step/srun to bind to different cpus.</p>
<p>This will produce 8 output files, 2 for each step:</p>
<ul class="simple">
<li><p>JOBID-step-0-task-0.out</p></li>
<li><p>JOBID-step-0-task-1.out</p></li>
<li><p>JOBID-step-1-task-0.out</p></li>
<li><p>JOBID-step-1-task-1.out</p></li>
<li><p>JOBID-step-2-task-0.out</p></li>
<li><p>JOBID-step-2-task-1.out</p></li>
<li><p>JOBID-step-3-task-0.out</p></li>
<li><p>JOBID-step-3-task-1.out</p></li>
</ul>
<p>Running <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> in silly.conf, while parsing the output, we can see 4
GPUs allocated and 2 tasks per GPU</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat JOBID-step-* <span class="p">|</span> grep Tesla
<span class="go">0: |   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |</span>
<span class="go">1: |   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |</span>
<span class="go">0: |   0  Tesla P100-PCIE...  On   | 00000000:83:00.0 Off |                    0 |</span>
<span class="go">1: |   0  Tesla P100-PCIE...  On   | 00000000:83:00.0 Off |                    0 |</span>
<span class="go">0: |   0  Tesla P100-PCIE...  On   | 00000000:82:00.0 Off |                    0 |</span>
<span class="go">1: |   0  Tesla P100-PCIE...  On   | 00000000:82:00.0 Off |                    0 |</span>
<span class="go">0: |   0  Tesla P100-PCIE...  On   | 00000000:03:00.0 Off |                    0 |</span>
<span class="go">1: |   0  Tesla P100-PCIE...  On   | 00000000:03:00.0 Off |                    0 |</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="multiple-nodes">
<h2>Multiple Nodes<a class="headerlink" href="#multiple-nodes" title="Link permanente para este cabeçalho">¶</a></h2>
<section id="data-parallel">
<h3>Data Parallel<a class="headerlink" href="#data-parallel" title="Link permanente para este cabeçalho">¶</a></h3>
<img alt="_images/dataparallel.png" src="_images/dataparallel.png" />
<p>Request 3 nodes with at least 4 GPUs each.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>!/bin/bash

<span class="gp"># </span>Number of Nodes
<span class="gp">#</span>SBATCH --nodes<span class="o">=</span><span class="m">3</span>

<span class="gp"># </span>Number of tasks. <span class="m">3</span> <span class="o">(</span><span class="m">1</span> per node<span class="o">)</span>
<span class="gp">#</span>SBATCH --ntasks<span class="o">=</span><span class="m">3</span>

<span class="gp"># </span>Number of GPU per node
<span class="gp">#</span>SBATCH --gres<span class="o">=</span>gpu:4
<span class="gp">#</span>SBATCH --gpus-per-node<span class="o">=</span><span class="m">4</span>

<span class="gp"># </span><span class="m">16</span> CPUs per node
<span class="gp">#</span>SBATCH --cpus-per-gpu<span class="o">=</span><span class="m">4</span>

<span class="gp"># </span>16Go per nodes <span class="o">(</span>4Go per GPU<span class="o">)</span>
<span class="gp">#</span>SBATCH --mem<span class="o">=</span>16G

<span class="gp"># </span>we need all nodes to be ready at the same <span class="nb">time</span>
<span class="gp">#</span>SBATCH --wait-all-nodes<span class="o">=</span><span class="m">1</span>

<span class="gp"># </span>Total resources:
<span class="gp">#   </span>CPU: <span class="m">16</span> * <span class="nv">3</span> <span class="o">=</span> <span class="m">48</span>
<span class="gp">#   </span>RAM: <span class="m">16</span> * <span class="nv">3</span> <span class="o">=</span> <span class="m">48</span> Go
<span class="gp">#   </span>GPU:  <span class="m">4</span> * <span class="nv">3</span> <span class="o">=</span> <span class="m">12</span>

<span class="gp"># </span>Setup our rendez-vous point
<span class="go">RDV_ADDR=$(hostname)</span>
<span class="go">WORLD_SIZE=$SLURM_JOB_NUM_NODES</span>
<span class="gp"># </span>-----

<span class="go">srun -l torchrun \</span>
<span class="go">    --nproc_per_node=$SLURM_GPUS_PER_NODE\</span>
<span class="go">    --nnodes=$WORLD_SIZE\</span>
<span class="go">    --rdzv_id=$SLURM_JOB_ID\</span>
<span class="go">    --rdzv_backend=c10d\</span>
<span class="go">    --rdzv_endpoint=$RDV_ADDR\</span>
<span class="go">    training_script.py</span>
</pre></div>
</div>
<p>You can find below a pytorch script outline on what a multi-node trainer could look like.</p>
<div class="highlight-python notranslate" id="training-script-outline-for-multi-node-training"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch.distributed</span> <span class="k">as</span> <span class="nn">dist</span>

<span class="k">class</span> <span class="nc">Trainer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chk_path</span> <span class="o">=</span> <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="o">...</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">device_id</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span>

    <span class="k">def</span> <span class="nf">load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chk_path</span> <span class="o">=</span> <span class="n">path</span>
        <span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">should_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Note: only one worker saves its weights</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">chk_path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Save your states here</span>
        <span class="c1"># Note: you should save the weights of self.model not ddp_model</span>
        <span class="c1"># ...</span>

    <span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;RANK&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_rank</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Global rank should be set (Only Rank 0 can save checkpoints)&#39;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_rank</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Local rank should be set&#39;</span>

        <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s2">&quot;gloo|nccl&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sync_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">resuming</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">resuming</span><span class="p">:</span>
            <span class="c1"># in the case of resuming all workers need to load the same checkpoint</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">()</span>

            <span class="c1"># Wait for everybody to finish loading the checkpoint</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>
            <span class="k">return</span>

        <span class="c1"># Make sure all workers have the same initial weights</span>
        <span class="c1"># This makes the leader save his weights</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_checkpoint</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">()</span>

        <span class="c1"># All workers wait for the leader to finish</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

        <span class="c1"># All followers load the leader&#39;s weights</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_checkpoint</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">()</span>

        <span class="c1"># Leader waits for the follower to load the weights</span>
        <span class="n">dist</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="n">train_sampler</span> <span class="o">=</span> <span class="n">ElasticDistributedSampler</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
        <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">dataset</span><span class="p">,</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
            <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">train_loader</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Your batch processing step here</span>
        <span class="c1"># ...</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">()</span>

        <span class="n">ddp_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
            <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">device_id</span><span class="p">],</span>
            <span class="n">output_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device_id</span>
        <span class="p">)</span>

        <span class="n">loader</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">iter</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">train_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_checkpoint</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">()</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">tainer</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>To bypass Python GIL (Global interpreter lock) pytorch spawn one process for each GPU.
In the example above this means at least 12 processes are spawn, at least 4 on each node.</p>
</div>
</section>
</section>
<section id="frequently-asked-questions-faqs">
<h2>Frequently asked questions (FAQs)<a class="headerlink" href="#frequently-asked-questions-faqs" title="Link permanente para este cabeçalho">¶</a></h2>
<section id="connection-ssh-issues">
<h3>Connection/SSH issues<a class="headerlink" href="#connection-ssh-issues" title="Link permanente para este cabeçalho">¶</a></h3>
<section id="i-m-getting-connection-refused-while-trying-to-connect-to-a-login-node">
<h4>I’m getting <code class="docutils literal notranslate"><span class="pre">connection</span> <span class="pre">refused</span></code> while trying to connect to a login node<a class="headerlink" href="#i-m-getting-connection-refused-while-trying-to-connect-to-a-login-node" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Login nodes are protected against brute force attacks and might ban your IP if
it detects too many connections/failures. You will be automatically unbanned
after 1 hour. For any further problem, please <a href="#id9"><span class="problematic" id="id10">`</span></a>submit a support ticket.</p>
</section>
</section>
<section id="shell-issues">
<h3>Shell issues<a class="headerlink" href="#shell-issues" title="Link permanente para este cabeçalho">¶</a></h3>
<section id="how-do-i-change-my-shell">
<h4>How do I change my shell ?<a class="headerlink" href="#how-do-i-change-my-shell" title="Link permanente para este cabeçalho">¶</a></h4>
<p>By default you will be assigned <code class="docutils literal notranslate"><span class="pre">/bin/bash</span></code> as a shell. If you would like to
change for another one, please <a href="#id11"><span class="problematic" id="id12">`</span></a>submit a support ticket.</p>
</section>
</section>
<section id="slurm-issues">
<h3>SLURM issues<a class="headerlink" href="#slurm-issues" title="Link permanente para este cabeçalho">¶</a></h3>
<section id="how-can-i-get-an-interactive-shell-on-the-cluster">
<h4>How can I get an interactive shell on the cluster ?<a class="headerlink" href="#how-can-i-get-an-interactive-shell-on-the-cluster" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Use <code class="docutils literal notranslate"><span class="pre">salloc</span> <span class="pre">[--slurm_options]</span></code> without any executable at the end of the
command, this will launch your default shell on an interactive session. Remember
that an interactive session is bound to the login node where you start it so you
could risk losing your job if the login node becomes unreachable.</p>
</section>
<section id="how-can-i-reset-my-cluster-password">
<h4>How can I reset my cluster password ?<a class="headerlink" href="#how-can-i-reset-my-cluster-password" title="Link permanente para este cabeçalho">¶</a></h4>
<p>To reset your password, please <a href="#id13"><span class="problematic" id="id14">`</span></a>submit a support ticket.</p>
<p><strong>Warning</strong>: your cluster password is the same as your Google Workspace account. So,
after reset, you must use the new password for all your Google services.</p>
</section>
<section id="srun-error-mem-and-mem-per-cpu-are-mutually-exclusive">
<h4>srun: error: –mem and –mem-per-cpu are mutually exclusive<a class="headerlink" href="#srun-error-mem-and-mem-per-cpu-are-mutually-exclusive" title="Link permanente para este cabeçalho">¶</a></h4>
<p>You can safely ignore this, <code class="docutils literal notranslate"><span class="pre">salloc</span></code> has a default memory flag in case you
don’t provide one.</p>
</section>
<section id="how-can-i-see-where-and-if-my-jobs-are-running">
<h4>How can I see where and if my jobs are running ?<a class="headerlink" href="#how-can-i-see-where-and-if-my-jobs-are-running" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Use <code class="docutils literal notranslate"><span class="pre">squeue</span> <span class="pre">-u</span> <span class="pre">YOUR_USERNAME</span></code> to see all your job status and locations.
To get more info on a running job, try <code class="docutils literal notranslate"><span class="pre">scontrol</span> <span class="pre">show</span> <span class="pre">job</span> <span class="pre">#JOBID</span></code></p>
</section>
<section id="unable-to-allocate-resources-invalid-account-or-account-partition-combination-specified">
<h4>Unable to allocate resources: Invalid account or account/partition combination specified<a class="headerlink" href="#unable-to-allocate-resources-invalid-account-or-account-partition-combination-specified" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Chances are your account is not setup properly. You should <a href="#id15"><span class="problematic" id="id16">`</span></a>submit a support ticket.</p>
</section>
<section id="how-do-i-cancel-a-job">
<h4>How do I cancel a job?<a class="headerlink" href="#how-do-i-cancel-a-job" title="Link permanente para este cabeçalho">¶</a></h4>
<ul class="simple">
<li><p>To cancel a specific job, use <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">#JOBID</span></code></p></li>
<li><p>To cancel all your jobs (running and pending), use <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">-u</span> <span class="pre">YOUR_USERNAME</span></code></p></li>
<li><p>To cancel all your pending jobs only, use <code class="docutils literal notranslate"><span class="pre">scancel</span> <span class="pre">-t</span> <span class="pre">PD</span></code></p></li>
</ul>
</section>
<section id="how-can-i-access-a-node-on-which-one-of-my-jobs-is-running">
<h4>How can I access a node on which one of my jobs is running ?<a class="headerlink" href="#how-can-i-access-a-node-on-which-one-of-my-jobs-is-running" title="Link permanente para este cabeçalho">¶</a></h4>
<p>You can ssh into a node on which you have a job running, your ssh connection
will be adopted by your job, i.e.  if your job finishes your ssh connection will
be automatically terminated. In order to connect to a node, you need to have
password-less ssh either with a key present in your home or with an
<code class="docutils literal notranslate"><span class="pre">ssh-agent</span></code>. You can generate a key on the login node like this:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ssh-keygen (3xENTER)</span>
<span class="go">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span>
<span class="go">chmod 600 ~/.ssh/authorized_keys</span>
<span class="go">chmod 700 ~/.ssh</span>
</pre></div>
</div>
</section>
<section id="i-m-getting-permission-denied-publickey-while-trying-to-connect-to-a-node">
<h4>I’m getting <code class="docutils literal notranslate"><span class="pre">Permission</span> <span class="pre">denied</span> <span class="pre">(publickey)</span></code> while trying to connect to a node<a class="headerlink" href="#i-m-getting-permission-denied-publickey-while-trying-to-connect-to-a-node" title="Link permanente para este cabeçalho">¶</a></h4>
<p>See previous question</p>
</section>
<section id="where-do-i-put-my-data-during-a-job">
<h4>Where do I put my data during a job ?<a class="headerlink" href="#where-do-i-put-my-data-during-a-job" title="Link permanente para este cabeçalho">¶</a></h4>
<p>Your <code class="docutils literal notranslate"><span class="pre">/home</span></code> as well as the datasets are on shared file-systems, it is
recommended to copy them to the <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> to better process them and
leverage higher-speed local drives. If you run a low priority job subject to
preemption, it’s better to save any output you want to keep on the shared file
systems, because the <code class="docutils literal notranslate"><span class="pre">$SLURM_TMPDIR</span></code> is deleted at the end of each job.</p>
</section>
<section id="slurmstepd-error-detected-1-oom-kill-event-s-in-step-batch-cgroup">
<h4>slurmstepd: error: Detected 1 oom-kill event(s) in step #####.batch cgroup<a class="headerlink" href="#slurmstepd-error-detected-1-oom-kill-event-s-in-step-batch-cgroup" title="Link permanente para este cabeçalho">¶</a></h4>
<p>You exceeded the amount of memory allocated to your job, either you did not
request enough memory or you have a memory leak in your process. Try increasing
the amount of memory requested with <code class="docutils literal notranslate"><span class="pre">--mem=</span></code> or <code class="docutils literal notranslate"><span class="pre">--mem-per-cpu=</span></code>.</p>
</section>
<section id="fork-retry-resource-temporarily-unavailable">
<h4>fork: retry: Resource temporarily unavailable<a class="headerlink" href="#fork-retry-resource-temporarily-unavailable" title="Link permanente para este cabeçalho">¶</a></h4>
<p>You exceeded the limit of 2000 tasks/PIDs in your job, it probably means there
is an issue with a sub-process spawning too many processes in your script. For
any help with your software, please <a href="#id17"><span class="problematic" id="id18">`</span></a>submit a support ticket.</p>
</section>
</section>
<section id="pytorch-issues">
<h3>PyTorch issues<a class="headerlink" href="#pytorch-issues" title="Link permanente para este cabeçalho">¶</a></h3>
<section id="i-randomly-get-internal-assert-failed-at-aten-src-aten-mapallocator-cpp-263">
<h4>I randomly get <code class="docutils literal notranslate"><span class="pre">INTERNAL</span> <span class="pre">ASSERT</span> <span class="pre">FAILED</span> <span class="pre">at</span> <span class="pre">&quot;../aten/src/ATen/MapAllocator.cpp&quot;:263</span></code><a class="headerlink" href="#i-randomly-get-internal-assert-failed-at-aten-src-aten-mapallocator-cpp-263" title="Link permanente para este cabeçalho">¶</a></h4>
<p>You are using PyTorch 1.10.x and hitting <a class="reference external" href="https://github.com/pytorch/pytorch/issues/67864">#67864</a>,
for which the solution is <a class="reference external" href="https://github.com/pytorch/pytorch/pull/72232">PR #72232</a>
merged in PyTorch 1.11.x. For an immediate fix, consider the following compilable Gist:
<a class="reference external" href="https://gist.github.com/obilaniu/b133470cb70410d841faca819d3921e5">hack.cpp</a>.
Compile the patch to <code class="docutils literal notranslate"><span class="pre">hack.so</span></code> and then <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">LD_PRELOAD=/absolute/path/to/hack.so</span></code>
before executing the Python process that <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">torch</span></code> a broken PyTorch 1.10.</p>
<p>For Hydra users who are using the submitit launcher plug-in, the <code class="docutils literal notranslate"><span class="pre">env_set</span></code> key cannot
be used to set <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> in the environment as it does so too late at runtime. The
dynamic loader reads <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> only once and very early during the startup of any
process, before the variable can be set from inside the process. The hack must therefore
be injected using the <code class="docutils literal notranslate"><span class="pre">setup</span></code> key in Hydra YAML config file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hydra</span><span class="p">:</span>
    <span class="n">launcher</span><span class="p">:</span>
        <span class="n">setup</span><span class="p">:</span>
            <span class="o">-</span> <span class="n">export</span> <span class="n">LD_PRELOAD</span><span class="o">=/</span><span class="n">absolute</span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">hack</span><span class="o">.</span><span class="n">so</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/cin-logo.png" alt="Logo"/>
            </a></p>
  <div>
    <h3><a href="index.html">Tabela de Conteúdo</a></h3>
    <ul>
<li><a class="reference internal" href="#">User’s guide</a><ul>
<li><a class="reference internal" href="#running-your-code">Running your code</a><ul>
<li><a class="reference internal" href="#slurm-commands-guide">SLURM commands guide</a><ul>
<li><a class="reference internal" href="#basic-usage">Basic Usage</a></li>
<li><a class="reference internal" href="#submitting-jobs">Submitting jobs</a><ul>
<li><a class="reference internal" href="#batch-job">Batch job</a></li>
<li><a class="reference internal" href="#interactive-job">Interactive job</a></li>
</ul>
</li>
<li><a class="reference internal" href="#job-submission-arguments">Job submission arguments</a></li>
<li><a class="reference internal" href="#checking-job-status">Checking job status</a></li>
<li><a class="reference internal" href="#removing-a-job">Removing a job</a></li>
</ul>
</li>
<li><a class="reference internal" href="#partitioning">Partitioning</a><ul>
<li><a class="reference internal" href="#information-on-partitions-nodes">Information on partitions/nodes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#useful-commands">Useful Commands</a></li>
<li><a class="reference internal" href="#special-gpu-requirements">Special GPU requirements</a></li>
<li><a class="reference internal" href="#example-script">Example script</a></li>
</ul>
</li>
<li><a class="reference internal" href="#portability-concerns-and-solutions">Portability concerns and solutions</a><ul>
<li><a class="reference internal" href="#managing-your-environments">Managing your environments</a></li>
<li><a class="reference internal" href="#virtual-environments">Virtual environments</a><ul>
<li><a class="reference internal" href="#pip-virtualenv">Pip/Virtualenv</a></li>
<li><a class="reference internal" href="#conda">Conda</a></li>
</ul>
</li>
<li><a class="reference internal" href="#using-modules">Using Modules</a><ul>
<li><a class="reference internal" href="#the-module-command">The module command</a></li>
<li><a class="reference internal" href="#available-software">Available Software</a></li>
<li><a class="reference internal" href="#default-package-location">Default package location</a></li>
</ul>
</li>
<li><a class="reference internal" href="#on-using-containers">On using containers</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id2">Singularity</a><ul>
<li><a class="reference internal" href="#overview">Overview</a><ul>
<li><a class="reference internal" href="#what-is-singularity">What is Singularity?</a></li>
<li><a class="reference internal" href="#links-to-official-documentation">Links to official documentation</a></li>
<li><a class="reference internal" href="#overview-of-the-steps-used-in-practice">Overview of the steps used in practice</a></li>
<li><a class="reference internal" href="#nope-not-on-macos">Nope, not on MacOS</a></li>
<li><a class="reference internal" href="#where-to-build-images">Where to build images</a></li>
</ul>
</li>
<li><a class="reference internal" href="#building-the-containers">Building the containers</a><ul>
<li><a class="reference internal" href="#first-way-build-and-use-a-sandbox">First way: Build and use a sandbox</a><ul>
<li><a class="reference internal" href="#download-containers-from-the-web">Download containers from the web</a></li>
<li><a class="reference internal" href="#how-to-add-or-install-stuff-in-a-container">How to add or install stuff in a container</a></li>
<li><a class="reference internal" href="#creating-useful-directories">Creating useful directories</a></li>
<li><a class="reference internal" href="#testing">Testing</a></li>
<li><a class="reference internal" href="#creating-a-new-image-from-the-sandbox">Creating a new image from the sandbox</a></li>
</ul>
</li>
<li><a class="reference internal" href="#second-way-use-recipes">Second way: Use recipes</a><ul>
<li><a class="reference internal" href="#build-recipe-on-singularity-hub">Build recipe on singularity hub</a></li>
<li><a class="reference internal" href="#example-recipe-with-openai-gym-mujoco-and-miniworld">Example: Recipe with OpenAI gym, MuJoCo and Miniworld</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#using-containers-on-clusters">Using containers on clusters</a><ul>
<li><a class="reference internal" href="#how-to-use-containers-on-clusters">How to use containers on clusters</a><ul>
<li><a class="reference internal" href="#example-interactive-case-srun-salloc">Example: Interactive case (srun/salloc)</a></li>
<li><a class="reference internal" href="#example-sbatch-case">Example: sbatch case</a></li>
<li><a class="reference internal" href="#issue-with-pybullet-and-opengl-libraries">Issue with PyBullet and OpenGL libraries</a></li>
<li><a class="reference internal" href="#apuana-cluster">Apuana cluster</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#sharing-data-with-acls">Sharing Data with ACLs</a><ul>
<li><a class="reference internal" href="#viewing-and-verifying-acls">Viewing and Verifying ACLs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advanced-slurm-usage-and-multiple-gpu-jobs">Advanced SLURM usage and Multiple GPU jobs</a><ul>
<li><a class="reference internal" href="#handling-preemption">Handling preemption</a></li>
<li><a class="reference internal" href="#packing-jobs">Packing jobs</a><ul>
<li><a class="reference internal" href="#sharing-a-gpu-between-processes">Sharing a GPU between processes</a></li>
<li><a class="reference internal" href="#sharing-a-node-with-multiple-gpu-1process-gpu">Sharing a node with multiple GPU 1process/GPU</a></li>
<li><a class="reference internal" href="#sharing-a-node-with-multiple-gpu-multiple-processes-gpu">Sharing a node with multiple GPU &amp; multiple processes/GPU</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#multiple-nodes">Multiple Nodes</a><ul>
<li><a class="reference internal" href="#data-parallel">Data Parallel</a></li>
</ul>
</li>
<li><a class="reference internal" href="#frequently-asked-questions-faqs">Frequently asked questions (FAQs)</a><ul>
<li><a class="reference internal" href="#connection-ssh-issues">Connection/SSH issues</a><ul>
<li><a class="reference internal" href="#i-m-getting-connection-refused-while-trying-to-connect-to-a-login-node">I’m getting <code class="docutils literal notranslate"><span class="pre">connection</span> <span class="pre">refused</span></code> while trying to connect to a login node</a></li>
</ul>
</li>
<li><a class="reference internal" href="#shell-issues">Shell issues</a><ul>
<li><a class="reference internal" href="#how-do-i-change-my-shell">How do I change my shell ?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#slurm-issues">SLURM issues</a><ul>
<li><a class="reference internal" href="#how-can-i-get-an-interactive-shell-on-the-cluster">How can I get an interactive shell on the cluster ?</a></li>
<li><a class="reference internal" href="#how-can-i-reset-my-cluster-password">How can I reset my cluster password ?</a></li>
<li><a class="reference internal" href="#srun-error-mem-and-mem-per-cpu-are-mutually-exclusive">srun: error: –mem and –mem-per-cpu are mutually exclusive</a></li>
<li><a class="reference internal" href="#how-can-i-see-where-and-if-my-jobs-are-running">How can I see where and if my jobs are running ?</a></li>
<li><a class="reference internal" href="#unable-to-allocate-resources-invalid-account-or-account-partition-combination-specified">Unable to allocate resources: Invalid account or account/partition combination specified</a></li>
<li><a class="reference internal" href="#how-do-i-cancel-a-job">How do I cancel a job?</a></li>
<li><a class="reference internal" href="#how-can-i-access-a-node-on-which-one-of-my-jobs-is-running">How can I access a node on which one of my jobs is running ?</a></li>
<li><a class="reference internal" href="#i-m-getting-permission-denied-publickey-while-trying-to-connect-to-a-node">I’m getting <code class="docutils literal notranslate"><span class="pre">Permission</span> <span class="pre">denied</span> <span class="pre">(publickey)</span></code> while trying to connect to a node</a></li>
<li><a class="reference internal" href="#where-do-i-put-my-data-during-a-job">Where do I put my data during a job ?</a></li>
<li><a class="reference internal" href="#slurmstepd-error-detected-1-oom-kill-event-s-in-step-batch-cgroup">slurmstepd: error: Detected 1 oom-kill event(s) in step #####.batch cgroup</a></li>
<li><a class="reference internal" href="#fork-retry-resource-temporarily-unavailable">fork: retry: Resource temporarily unavailable</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch-issues">PyTorch issues</a><ul>
<li><a class="reference internal" href="#i-randomly-get-internal-assert-failed-at-aten-src-aten-mapallocator-cpp-263">I randomly get <code class="docutils literal notranslate"><span class="pre">INTERNAL</span> <span class="pre">ASSERT</span> <span class="pre">FAILED</span> <span class="pre">at</span> <span class="pre">&quot;../aten/src/ATen/MapAllocator.cpp&quot;:263</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Tópico anterior</h4>
    <p class="topless"><a href="information.html"
                          title="capítulo anterior">Computing infrastructure and policies</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>Essa Página</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/Userguide.rst.txt"
            rel="nofollow">Exibir Fonte</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Busca rápida</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Ir" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navegação</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="Índice Geral"
             >índice</a></li>
        <li class="right" >
          <a href="information.html" title="Computing infrastructure and policies"
             >anterior</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">documentação Cluster Cin latest</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">User’s guide</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2022.
      Criada usando <a href="https://www.sphinx-doc.org/pt_BR/master">Sphinx</a> 5.3.0.
    </div>
  </body>
</html>