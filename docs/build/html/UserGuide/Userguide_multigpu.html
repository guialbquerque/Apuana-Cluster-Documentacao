
<!DOCTYPE html>

<html lang="pt-BR">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Advanced SLURM usage and Multiple GPU jobs &#8212; documentação Cluster Cin latest</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/groundwork.css" />
    <script src="../_static/jquery.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/translations.js"></script>
    <link rel="index" title="Índice" href="../genindex.html" />
    <link rel="search" title="Buscar" href="../search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navegação</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="Índice Geral"
             accesskey="I">índice</a></li>
        <li class="nav-item nav-item-0"><a href="../index.html">documentação Cluster Cin latest</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Advanced SLURM usage and Multiple GPU jobs</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="advanced-slurm-usage-and-multiple-gpu-jobs">
<h1>Advanced SLURM usage and Multiple GPU jobs<a class="headerlink" href="#advanced-slurm-usage-and-multiple-gpu-jobs" title="Link permanente para este cabeçalho"></a></h1>
<section id="handling-preemption">
<h2>Handling preemption<a class="headerlink" href="#handling-preemption" title="Link permanente para este cabeçalho"></a></h2>
<p id="advanced-preemption">On the Apuana cluster, jobs can preempt one-another depending on their priority
(unkillable&gt;high&gt;low) (See the <a class="reference external" href="https://slurm.schedmd.com/preempt.html">Slurm documentation</a>)</p>
<p>The default preemption mechanism is to kill and re-queue the job automatically
without any notice. To allow a different preemption mechanism, every partition
have been duplicated (i.e. have the same characteristics as their counterparts)
allowing a <strong>120sec</strong> grace period before killing your job <em>but don’t requeue
it automatically</em>: those partitions are referred by the suffix: <code class="docutils literal notranslate"><span class="pre">-grace</span></code>
(<code class="docutils literal notranslate"><span class="pre">main-grace,</span> <span class="pre">long-grace,</span> <span class="pre">main-cpu-grace,</span> <span class="pre">long-cpu-grace</span></code>).</p>
<p>When using a partition with a grace period, a series of signals consisting of
first <code class="docutils literal notranslate"><span class="pre">SIGCONT</span></code> and <code class="docutils literal notranslate"><span class="pre">SIGTERM</span></code> then <code class="docutils literal notranslate"><span class="pre">SIGKILL</span></code> will be sent to the SLURM
job.  It’s good practice to catch those signals using the Linux <code class="docutils literal notranslate"><span class="pre">trap</span></code> command
to properly terminate a job and save what’s necessary to restart the job.  On
each cluster, you’ll be allowed a <em>grace period</em> before SLURM actually kills
your job (<code class="docutils literal notranslate"><span class="pre">SIGKILL</span></code>).</p>
<p>The easiest way to handle preemption is by trapping the <code class="docutils literal notranslate"><span class="pre">SIGTERM</span></code> signal</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>SBATCH<span class="w"> </span>--ntasks<span class="o">=</span><span class="m">1</span>
<span class="gp">#</span>SBATCH<span class="w"> </span>....

<span class="go">exit_script() {</span>
<span class="go">    echo &quot;Preemption signal, saving myself&quot;</span>
<span class="go">    trap - SIGTERM # clear the trap</span>
<span class="gp">    # </span>Optional:<span class="w"> </span>sends<span class="w"> </span>SIGTERM<span class="w"> </span>to<span class="w"> </span>child/sub<span class="w"> </span>processes
<span class="go">    kill -- -$$</span>
<span class="go">}</span>

<span class="go">trap exit_script SIGTERM</span>

<span class="gp"># </span>The<span class="w"> </span>main<span class="w"> </span>script<span class="w"> </span>part
<span class="go">python3 my_script</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<div class="line-block">
<div class="line"><strong>Requeuing</strong>:</div>
<div class="line">The Slurm scheduler on the cluster does not allow a grace period before</div>
<div class="line">preempting a job while requeuing it automatically, therefore your job will</div>
<div class="line">be cancelled at the end of the grace period.</div>
<div class="line">To automatically requeue it, you can just add the <code class="docutils literal notranslate"><span class="pre">sbatch</span></code> command inside</div>
<div class="line">your <code class="docutils literal notranslate"><span class="pre">exit_script</span></code> function.</div>
</div>
</div>
</section>
<section id="packing-jobs">
<h2>Packing jobs<a class="headerlink" href="#packing-jobs" title="Link permanente para este cabeçalho"></a></h2>
<section id="sharing-a-gpu-between-processes">
<h3>Sharing a GPU between processes<a class="headerlink" href="#sharing-a-gpu-between-processes" title="Link permanente para este cabeçalho"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">srun</span></code>, when used in a batch job is responsible for starting tasks on the
allocated resources (see srun) SLURM batch script</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>SBATCH<span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">2</span>
<span class="gp">#</span>SBATCH<span class="w"> </span>--output<span class="o">=</span>myjob_output_wrapper.out
<span class="gp">#</span>SBATCH<span class="w"> </span>--ntasks<span class="o">=</span><span class="m">2</span>
<span class="gp">#</span>SBATCH<span class="w"> </span>--gres<span class="o">=</span>gpu:1
<span class="gp">#</span>SBATCH<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">4</span>
<span class="gp">#</span>SBATCH<span class="w"> </span>--mem<span class="o">=</span>18G
<span class="go">srun -l --output=myjob_output_%t.out python script args</span>
</pre></div>
</div>
<p>This will run Python 2 times, each process with 4 CPUs with the same arguments
<code class="docutils literal notranslate"><span class="pre">--output=myjob_output_%t.out</span></code> will create 2 output files appending the task
id (<code class="docutils literal notranslate"><span class="pre">%t</span></code>) to the filename and 1 global log file for things happening outside
the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command.</p>
<p>Knowing that, if you want to have 2 different arguments to the Python program,
you can use a multi-prog configuration file: <code class="docutils literal notranslate"><span class="pre">srun</span> <span class="pre">-l</span> <span class="pre">--multi-prog</span> <span class="pre">silly.conf</span></code></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">0  python script firstarg</span>
<span class="go">1  python script secondarg</span>
</pre></div>
</div>
<p>Or by specifying a range of tasks</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">0-1  python script %t</span>
</pre></div>
</div>
<p>%t being the taskid that your Python script will parse.  Note the <code class="docutils literal notranslate"><span class="pre">-l</span></code> on the
<code class="docutils literal notranslate"><span class="pre">srun</span></code> command: this will prepend each line with the taskid (0:, 1:)</p>
</section>
<section id="sharing-a-node-with-multiple-gpu-1process-gpu">
<h3>Sharing a node with multiple GPU 1process/GPU<a class="headerlink" href="#sharing-a-node-with-multiple-gpu-1process-gpu" title="Link permanente para este cabeçalho"></a></h3>
<p>On Digital Research Alliance of Canada, several nodes, especially nodes with
<code class="docutils literal notranslate"><span class="pre">largeGPU</span></code> (P100) are reserved for jobs requesting the whole node, therefore
packing multiple processes in a single job can leverage faster GPU.</p>
<p>If you want different tasks to access different GPUs in a single allocation you
need to create an allocation requesting a whole node and using <code class="docutils literal notranslate"><span class="pre">srun</span></code> with a
subset of those resources (1 GPU).</p>
<p>Keep in mind that every resource not specified on the <code class="docutils literal notranslate"><span class="pre">srun</span></code> command while
inherit the global allocation specification so you need to split each resource
in a subset (except –cpu-per-task which is a per-task requirement)</p>
<p>Each <code class="docutils literal notranslate"><span class="pre">srun</span></code> represents a job step (<code class="docutils literal notranslate"><span class="pre">%s</span></code>).</p>
<p>Example for a GPU node with 24 cores and 4 GPUs and 128G of RAM
Requesting 1 task per GPU</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>!/bin/bash
<span class="gp">#</span>SBATCH<span class="w"> </span>--nodes<span class="o">=</span><span class="m">1</span>-1
<span class="gp">#</span>SBATCH<span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">4</span>
<span class="gp">#</span>SBATCH<span class="w"> </span>--output<span class="o">=</span>myjob_output_wrapper.out
<span class="gp">#</span>SBATCH<span class="w"> </span>--gres<span class="o">=</span>gpu:4
<span class="gp">#</span>SBATCH<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">6</span>
<span class="go">srun --gres=gpu:1 -n1 --mem=30G -l --output=%j-step-%s.out --exclusive --multi-prog python script args1 &amp;</span>
<span class="go">srun --gres=gpu:1 -n1 --mem=30G -l --output=%j-step-%s.out --exclusive --multi-prog python script args2 &amp;</span>
<span class="go">srun --gres=gpu:1 -n1 --mem=30G -l --output=%j-step-%s.out --exclusive --multi-prog python script args3 &amp;</span>
<span class="go">srun --gres=gpu:1 -n1 --mem=30G -l --output=%j-step-%s.out --exclusive --multi-prog python script args4 &amp;</span>
<span class="go">wait</span>
</pre></div>
</div>
<p>This will create 4 output files:</p>
<ul class="simple">
<li><p>JOBID-step-0.out</p></li>
<li><p>JOBID-step-1.out</p></li>
<li><p>JOBID-step-2.out</p></li>
<li><p>JOBID-step-3.out</p></li>
</ul>
</section>
<section id="sharing-a-node-with-multiple-gpu-multiple-processes-gpu">
<h3>Sharing a node with multiple GPU &amp; multiple processes/GPU<a class="headerlink" href="#sharing-a-node-with-multiple-gpu-multiple-processes-gpu" title="Link permanente para este cabeçalho"></a></h3>
<p>Combining both previous sections, we can create a script requesting a whole node
with four GPUs, allocating 1 GPU per <code class="docutils literal notranslate"><span class="pre">srun</span></code> and sharing each GPU with multiple
processes</p>
<p>Example still with a 24 cores/4 GPUs/128G RAM
Requesting 2 tasks per GPU</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">#</span>!/bin/bash
<span class="gp">#</span>SBATCH<span class="w"> </span>--nodes<span class="o">=</span><span class="m">1</span>-1
<span class="gp">#</span>SBATCH<span class="w"> </span>--ntasks-per-node<span class="o">=</span><span class="m">8</span>
<span class="gp">#</span>SBATCH<span class="w"> </span>--output<span class="o">=</span>myjob_output_wrapper.out
<span class="gp">#</span>SBATCH<span class="w"> </span>--gres<span class="o">=</span>gpu:4
<span class="gp">#</span>SBATCH<span class="w"> </span>--cpus-per-task<span class="o">=</span><span class="m">3</span>
<span class="go">srun --gres=gpu:1 -n2 --mem=30G -l --output=%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf &amp;</span>
<span class="go">srun --gres=gpu:1 -n2 --mem=30G -l --output=%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf &amp;</span>
<span class="go">srun --gres=gpu:1 -n2 --mem=30G -l --output=%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf &amp;</span>
<span class="go">srun --gres=gpu:1 -n2 --mem=30G -l --output=%j-step-%s-task-%t.out --exclusive --multi-prog silly.conf &amp;</span>
<span class="go">wait</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">--exclusive</span></code> is important to specify subsequent step/srun to bind to different cpus.</p>
<p>This will produce 8 output files, 2 for each step:</p>
<ul class="simple">
<li><p>JOBID-step-0-task-0.out</p></li>
<li><p>JOBID-step-0-task-1.out</p></li>
<li><p>JOBID-step-1-task-0.out</p></li>
<li><p>JOBID-step-1-task-1.out</p></li>
<li><p>JOBID-step-2-task-0.out</p></li>
<li><p>JOBID-step-2-task-1.out</p></li>
<li><p>JOBID-step-3-task-0.out</p></li>
<li><p>JOBID-step-3-task-1.out</p></li>
</ul>
<p>Running <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> in silly.conf, while parsing the output, we can see 4
GPUs allocated and 2 tasks per GPU</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>cat<span class="w"> </span>JOBID-step-*<span class="w"> </span><span class="p">|</span><span class="w"> </span>grep<span class="w"> </span>Tesla
<span class="go">0: |   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |</span>
<span class="go">1: |   0  Tesla P100-PCIE...  On   | 00000000:04:00.0 Off |                    0 |</span>
<span class="go">0: |   0  Tesla P100-PCIE...  On   | 00000000:83:00.0 Off |                    0 |</span>
<span class="go">1: |   0  Tesla P100-PCIE...  On   | 00000000:83:00.0 Off |                    0 |</span>
<span class="go">0: |   0  Tesla P100-PCIE...  On   | 00000000:82:00.0 Off |                    0 |</span>
<span class="go">1: |   0  Tesla P100-PCIE...  On   | 00000000:82:00.0 Off |                    0 |</span>
<span class="go">0: |   0  Tesla P100-PCIE...  On   | 00000000:03:00.0 Off |                    0 |</span>
<span class="go">1: |   0  Tesla P100-PCIE...  On   | 00000000:03:00.0 Off |                    0 |</span>
</pre></div>
</div>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/cin-logo.png" alt="Logo"/>
            </a></p>
  <div>
    <h3><a href="../index.html">Tabela de Conteúdo</a></h3>
    <ul>
<li><a class="reference internal" href="#">Advanced SLURM usage and Multiple GPU jobs</a><ul>
<li><a class="reference internal" href="#handling-preemption">Handling preemption</a></li>
<li><a class="reference internal" href="#packing-jobs">Packing jobs</a><ul>
<li><a class="reference internal" href="#sharing-a-gpu-between-processes">Sharing a GPU between processes</a></li>
<li><a class="reference internal" href="#sharing-a-node-with-multiple-gpu-1process-gpu">Sharing a node with multiple GPU 1process/GPU</a></li>
<li><a class="reference internal" href="#sharing-a-node-with-multiple-gpu-multiple-processes-gpu">Sharing a node with multiple GPU &amp; multiple processes/GPU</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>Essa Página</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/UserGuide/Userguide_multigpu.rst.txt"
            rel="nofollow">Exibir Fonte</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Busca rápida</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Ir" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navegação</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="Índice Geral"
             >índice</a></li>
        <li class="nav-item nav-item-0"><a href="../index.html">documentação Cluster Cin latest</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Advanced SLURM usage and Multiple GPU jobs</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2023.
      Criada usando <a href="https://www.sphinx-doc.org/pt_BR/master">Sphinx</a> 6.1.3.
    </div>
  </body>
</html>